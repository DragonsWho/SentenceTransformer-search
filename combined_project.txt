
==================================================
File: components/grok3_api.py
==================================================

# components/grok3_api.py

import time
import pyperclip
import subprocess
import os
import cv2
import numpy as np
import mss
import mimetypes
from functools import wraps

# Constants
TEMPLATES_DIR = "grok_templates"
WINDOW_ID_FILE = "grok_window_id.txt"
XDOTOOL = "xdotool"

def retry_on_failure(func):
    """Decorator to retry a function on failure."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception:
            return kwargs.get('default', None)
    return wrapper

def wait_for_condition(condition_func, timeout=5.0, interval=0.1):
    """Wait for a condition to be met within a timeout."""
    start = time.time()
    while time.time() - start < timeout:
        if result := condition_func():
            return result
        time.sleep(interval)
    return None

class XdotoolWrapper:
    """Wrapper for xdotool commands."""
    @staticmethod
    @retry_on_failure
    def run(command, default=None):
        return subprocess.run([XDOTOOL] + command, check=True, capture_output=True).returncode == 0

    @staticmethod
    @retry_on_failure
    def get_output(command):
        return subprocess.check_output([XDOTOOL] + command).decode('utf-8').strip()

class GrokAPI:
    def __init__(self, url="https://grok.com", reuse_window=False, anonymous_chat=False):
        os.makedirs(TEMPLATES_DIR, exist_ok=True)
        self.url = url
        self.reuse_window = reuse_window
        self.anonymous_chat = anonymous_chat
        self.window_id = None
        self.templates = self._load_templates()
        self.template_cache = self._preload_templates()

    def _preload_templates(self):
        """Preload UI templates into memory."""
        cache = {}
        for key, path in self.templates.items():
            try:
                img = cv2.imread(path)
                if img is not None:
                    cache[key] = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            except Exception:
                pass
        return cache

    def _load_templates(self):
        """Load UI template paths from file."""
        templates_path = os.path.join(TEMPLATES_DIR, "templates_info.txt")
        try:
            with open(templates_path, 'r') as f:
                templates = dict(line.strip().split(":", 1) for line in f if ":" in line)
            valid_keys = ['input_field', 'input_field_alt', 'copy_button', 
                         'copy_button_alt', 'send_button_active']
            return {k: v.strip() for k, v in templates.items() if k in valid_keys and os.path.exists(v.strip())}
        except FileNotFoundError:
            return {}

    def _save_window_id(self, window_id):
        with open(WINDOW_ID_FILE, 'w') as f:
            f.write(str(window_id))

    def _load_window_id(self):
        try:
            with open(WINDOW_ID_FILE, 'r') as f:
                return int(f.read().strip())
        except (FileNotFoundError, ValueError):
            return None

    def _open_browser(self):
        """Open a browser window and return its ID."""
        if self.reuse_window and (wid := self._load_window_id()) and XdotoolWrapper.run(['windowactivate', str(wid)]):
            return wid

        for browser in ["google-chrome", "chromium", "firefox"]:
            try:
                with open(os.devnull, 'w') as devnull:
                    subprocess.Popen([browser, "--new-window", self.url], stdout=devnull, stderr=devnull)
                time.sleep(2)
                self.window_id = int(XdotoolWrapper.get_output(['getactivewindow']))
                self._save_window_id(self.window_id)
                return self.window_id
            except Exception:
                continue
        return None

    def _capture_screenshot(self):
        """Capture a screenshot of the active window."""
        with mss.mss() as sct:
            if not self.window_id:
                return np.array(sct.grab(sct.monitors[1]))
            try:
                window_info = XdotoolWrapper.get_output(['getwindowgeometry', str(self.window_id)]).split('\n')
                x, y = map(int, window_info[1].split('Position: ')[1].split(' ')[0].split(','))
                w, h = map(int, window_info[2].split('Geometry: ')[1].split('x'))
                monitor = {"top": y, "left": x, "width": w, "height": h}
                return np.array(sct.grab(monitor))
            except Exception:
                return np.array(sct.grab(sct.monitors[1]))

    def _find_template(self, template_key, confidence=0.8):
        """Locate a template in the current screenshot."""
        screenshot = self._capture_screenshot()
        screenshot_rgb = cv2.cvtColor(screenshot, cv2.COLOR_BGR2RGB)
        template_rgb = self.template_cache.get(template_key)
        if template_rgb is None:
            return None
        h, w = template_rgb.shape[:-1]
        result = cv2.matchTemplate(screenshot_rgb, template_rgb, cv2.TM_CCOEFF_NORMED)
        _, max_val, _, max_loc = cv2.minMaxLoc(result)
        if max_val >= confidence:
            return max_loc[0] + w // 2, max_loc[1] + h // 2
        return None

    def _wait_for_template(self, template_key, alt_key=None, timeout=3.0, interval=0.5, confidence=0.7):
        """Wait for a template to appear."""
        templates = [template_key] + ([alt_key] if alt_key in self.templates else [])
        return wait_for_condition(lambda: next((pos for t in templates if (pos := self._find_template(t, confidence))), None), timeout, interval)

    def _copy_file(self, file_path):
        """Copy a file to the clipboard."""
        if not os.path.exists(file_path):
            return False
        mime_type, _ = mimetypes.guess_type(file_path)
        if mime_type and mime_type.startswith('image/'):
            try:
                subprocess.run(f'convert "{file_path}" png:- | xclip -selection clipboard -t image/png', shell=True, check=True, timeout=10)
                return True
            except Exception:
                return False
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                pyperclip.copy(f.read())
            return True
        except Exception:
            return False

    def send_message(self, message="", file_paths=None):
        """Send a message with optional files."""
        if not self.templates:
            return False
        input_pos = self._wait_for_template('input_field', 'input_field_alt', timeout=3.0)
        if not input_pos:
            XdotoolWrapper.run(['click', '5'])
            input_pos = self._wait_for_template('input_field', 'input_field_alt', timeout=3.0)
        if not input_pos:
            return False

        XdotoolWrapper.run(['mousemove', str(input_pos[0]), str(input_pos[1]), 'click', '1'])
        time.sleep(0.3)
        XdotoolWrapper.run(['key', 'ctrl+a', 'Delete'])

        # Открываем новый анонимный чат, если включён режим
        if self.anonymous_chat:
            XdotoolWrapper.run(['key', 'ctrl+shift+J'])
            time.sleep(0.3)  # Ждём загрузки интерфейса

        original_clipboard = pyperclip.paste()
        if message:
            pyperclip.copy(message)
            XdotoolWrapper.run(['key', 'ctrl+v'])
            time.sleep(1)

        if file_paths:
            for file_path in file_paths:
                if self._copy_file(file_path):
                    XdotoolWrapper.run(['key', 'ctrl+v'])
                    time.sleep(2.0)

        send_pos = self._wait_for_template('send_button_active', timeout=15.0, interval=0.5, confidence=0.8)
        if send_pos:
            XdotoolWrapper.run(['mousemove', str(send_pos[0]), str(send_pos[1]), 'click', '1'])
            pyperclip.copy(original_clipboard)
            return True
        pyperclip.copy(original_clipboard)
        return False



    def get_response(self, timeout=60):
        """Retrieve the response from the UI."""
        start_time = time.time()
        original_clipboard = pyperclip.paste()
        
        # Убеждаемся, что окно активно
        if self.window_id:
            XdotoolWrapper.run(['windowactivate', str(self.window_id)])
            time.sleep(0.1)  # Даем время на активацию окна
        
        while time.time() - start_time < timeout:
            for key in ['copy_button', 'copy_button_alt']:
                if key in self.templates and (pos := self._find_template(key, 0.7)):
                    XdotoolWrapper.run(['mousemove', str(pos[0]), str(pos[1]), 'click', '1'])
                    time.sleep(1)
                    response = pyperclip.paste()
                    if response != original_clipboard and response.strip():
                        return response
            
            # Переключаем фокус на основное окно чата
            time.sleep(1)  # Небольшая задержка перед кликом
            XdotoolWrapper.run(['click', '1'])  # Клик в текущей позиции курсора
            time.sleep(0.1)  # Небольшая задержка после клика

            XdotoolWrapper.run(['key', 'End'])  # Прокручиваем страницу
            time.sleep(1)
        
        return "Error: Timeout waiting for response"
    



    def ask(self, message="", file_paths=None, timeout=60, close_after=True):
        """Send a request and get a response."""
        if not self._open_browser():
            return "Error: Failed to open browser"
        if not self.send_message(message, file_paths):
            return "Error: Failed to send message"
        response = self.get_response(timeout)
        # Окно закрывается только если reuse_window=False и close_after=True
        if not self.reuse_window and close_after and (wid := self._load_window_id()):
            XdotoolWrapper.run(['windowactivate', str(wid), 'key', 'ctrl+F4'])
            os.remove(WINDOW_ID_FILE) if os.path.exists(WINDOW_ID_FILE) else None
        return response

def check_dependencies():
    """Verify required dependencies are installed."""
    return all(subprocess.run(['which', cmd], capture_output=True).returncode == 0 for cmd in ['xdotool', 'xclip', 'convert'])

if __name__ == "__main__":
    import sys
    args = sys.argv[1:]
    reuse_window = any(x in args for x in ["--reuse-window", "-rw"])
    anonymous_chat = any(x in args for x in ["--anonymous-chat", "-ac"])
    close_after = not any(x in args for x in ["--no-close", "-nc"])

    if not check_dependencies():
        print("Error: Missing dependencies (xdotool, xclip, imagemagick)")
        sys.exit(1)

    message = ""
    file_paths = []
    for arg in args:
        if arg.startswith("-"):
            continue
        elif os.path.exists(arg):
            file_paths.append(os.path.abspath(arg))
        elif not message:
            message = arg

    api = GrokAPI(reuse_window=reuse_window, anonymous_chat=anonymous_chat)
    if message or file_paths:
        response = api.ask(message, file_paths, close_after=close_after)
        print(response)
    else:
        print("Usage: python grok_api.py [options] \"message\" [files...]")
        print("Options: --reuse-window/-rw, --anonymous-chat/-ac, --no-close/-nc")

==================================================
File: components/api_authors.py
==================================================

# components/api_authors.py

import os
from dotenv import load_dotenv
import requests

load_dotenv()

class AuthorLister:
    def __init__(self):
        self.base_url = 'https://cyoa.cafe/api'
        self.email = os.getenv('EMAIL')
        self.password = os.getenv('PASSWORD')
        self.token = None
        
    def login(self):
        """Authenticate with the API"""
        try:
            response = requests.post(
                f'{self.base_url}/collections/users/auth-with-password',
                json={
                    'identity': self.email,
                    'password': self.password
                }
            )
            response.raise_for_status()
            self.token = response.json()['token']
            return True
        except Exception:
            return False
    
    def get_all_authors(self):
        """Get all existing authors and return as a comma-separated list"""
        try:
            if not self.token:
                raise Exception("Not authenticated")
                
            headers = {
                'Authorization': self.token
            }
            
            # Get all authors (with pagination)
            all_authors = []
            page = 1
            per_page = 200
            
            while True:
                response = requests.get(
                    f'{self.base_url}/collections/authors/records',
                    headers=headers,
                    params={'page': page, 'perPage': per_page}
                )
                response.raise_for_status()
                
                data = response.json()
                authors_chunk = data.get('items', [])
                all_authors.extend(authors_chunk)
                
                # Check if there are more pages
                if len(authors_chunk) < per_page:
                    break
                    
                page += 1
            
            # Extract author names
            author_names = [author['name'] for author in all_authors]
            
            # Sort alphabetically
            author_names.sort()
            
            # Return comma-separated list
            return ", ".join(author_names)
            
        except Exception:
            return ""

def main():
    lister = AuthorLister()
    
    # Authenticate
    if lister.login():
        # Get and print authors
        authors_list = lister.get_all_authors()
        print(authors_list)

if __name__ == "__main__":
    main()

==================================================
File: components/api_tags.py
==================================================

# components/api_tags.py

import os
import json
from dotenv import load_dotenv
import requests

load_dotenv()

class TagCategoriesLister:
    def __init__(self):
        self.base_url = 'https://cyoa.cafe/api'
        self.email = os.getenv('EMAIL')
        self.password = os.getenv('PASSWORD')
        self.token = None
        self.existing_tags = {}

    def login(self):
        try:
            response = requests.post(
                f'{self.base_url}/collections/users/auth-with-password',
                json={
                    'identity': self.email,
                    'password': self.password
                }
            )
            response.raise_for_status()
            self.token = response.json()['token']
            return True
        except Exception:
            return False

    def get_all_tags(self):
        try:
            if not self.token:
                return False

            headers = {
                'Authorization': self.token
            }
            
            all_tags = []
            page = 1
            per_page = 200
            
            while True:
                response = requests.get(
                    f'{self.base_url}/collections/tags/records',
                    headers=headers,
                    params={'page': page, 'perPage': per_page}
                )
                response.raise_for_status()
                
                data = response.json()
                tags_chunk = data.get('items', [])
                all_tags.extend(tags_chunk)
                
                if len(tags_chunk) < per_page:
                    break
                    
                page += 1
            
            for tag in all_tags:
                self.existing_tags[tag['id']] = {
                    'name': tag['name']
                }
            
            return True
        except Exception:
            return False

    def get_tag_categories(self):
        try:
            if not self.token:
                return None

            if not self.existing_tags:
                self.get_all_tags()
                
            headers = {
                'Authorization': self.token
            }
            
            response = requests.get(
                f'{self.base_url}/collections/tag_categories/records',
                headers=headers,
                params={'perPage': 100}
            )
            response.raise_for_status()
            
            categories_data = response.json().get('items', [])
            
            export_data = []
            
            for category in categories_data:
                cat_data = {
                    'category_name': category['name'],
                    'tags': []
                }
                
                tag_ids = category.get('tags', [])
                for tag_id in tag_ids:
                    if tag_id in self.existing_tags:
                        cat_data['tags'].append(self.existing_tags[tag_id]['name'])
                
                export_data.append(cat_data)
            
            categorized_tag_ids = set()
            for category in categories_data:
                categorized_tag_ids.update(category.get('tags', []))
            
            uncategorized_tags = []
            for tag_id, tag_info in self.existing_tags.items():
                if tag_id not in categorized_tag_ids:
                    uncategorized_tags.append(tag_info['name'])
            
            if uncategorized_tags:
                export_data.append({
                    'category_name': 'Uncategorized',
                    'tags': uncategorized_tags
                })
            
            return export_data
        except Exception:
            return None

def main():
    lister = TagCategoriesLister()
    
    if lister.login():
        categories = lister.get_tag_categories()
        if categories:
            print(json.dumps(categories, ensure_ascii=False))

if __name__ == "__main__":
    main()

==================================================
File: components/crawler.py
==================================================

# components/crawler.py

import requests
import json
import os

def json_to_md(data):
    """Convert JSON data to Markdown format"""
    md_content = []
    
    # Extract main content
    if 'rows' in data:
        for row in data['rows']:
            if 'titleText' in row:
                md_content.append(f"## {row.get('title', '')}\n")
                md_content.append(f"{row['titleText']}\n")
            
            if 'objects' in row:
                for obj in row['objects']:
                    if 'title' in obj:
                        md_content.append(f"### {obj['title']}\n")
                    if 'text' in obj:
                        md_content.append(f"{obj['text']}\n")
                    # Skip image links
                    pass
    
    return "\n".join(md_content)

def crawl_url(url):
    """Process single URL: download JSON and convert to Markdown
    
    Args:
        url (str): URL to the project.json file
        
    Returns:
        str: Path to the created markdown file or None if processing failed
    """
    try:
        # Fetch JSON data
        response = requests.get(url)
        data = response.json()
        
        # Convert to Markdown
        md_content = json_to_md(data)
        
        # Generate filename from URL
        project_name = url.split('/')[-2]
        
        # Make sure markdown directory exists
        os.makedirs("markdown", exist_ok=True)
        
        md_filename = f"markdown/{project_name}.md"
        
        # Extract game title from URL
        game_title = url.split('/')[-2].replace('_', ' ')
        
        # Get game URL without 'project.json'
        game_url = '/'.join(url.split('/')[:-1]) + '/'  # Отсекаем последний сегмент и добавляем слеш
        
        # Add game URL and title (marked as possible) to the beginning of markdown content
        md_content = f"Game URL: {game_url}\n\nPossible title: {game_title}\n\n{md_content}"
        
        # Save Markdown
        with open(md_filename, 'w', encoding='utf-8') as f:
            f.write(md_content)
            
        return md_filename
        
    except requests.exceptions.RequestException as e:
        print(f"Network error processing {url}: {str(e)}")
        return None
    except json.JSONDecodeError as e:
        print(f"JSON parsing error for {url}: {str(e)}")
        return None
    except Exception as e:
        print(f"Error processing {url}: {str(e)}")
        return None

def main():
    import sys
    if len(sys.argv) != 2:
        print("Usage: python -m components.crawler <url>")
        sys.exit(1)
        
    url = sys.argv[1]
    result = crawl_url(url)
    
    if result:
        print(f"Successfully processed URL. Output saved to {result}")
    else:
        print(f"Failed to process URL: {url}")
        sys.exit(1)

if __name__ == "__main__":
    main()

==================================================
File: components/game_checker.py
==================================================

# components/game_checker.py

import os
from dotenv import load_dotenv
import requests
import logging

load_dotenv()

class GameChecker:
    def __init__(self):
        self.base_url = 'https://cyoa.cafe/api'
        self.email = os.getenv('EMAIL')
        self.password = os.getenv('PASSWORD')
        self.token = None
        self.existing_games = {}  # Словарь для хранения игр: {link: game_data}
        self.logger = logging.getLogger(__name__)
        self.logger.info("GameChecker initialized")

    def login(self):
        """Аутентификация с API"""
        self.logger.info(f"Attempting to login with email: {self.email}")
        try:
            response = requests.post(
                f'{self.base_url}/collections/users/auth-with-password',
                json={
                    'identity': self.email,
                    'password': self.password
                }
            )
            response.raise_for_status()
            self.token = response.json()['token']
            self.logger.info("Successfully authenticated with API")
            return True
        except Exception as e:
            self.logger.error(f"Failed to authenticate with API: {str(e)}")
            return False

    def load_existing_games(self):
        """Загрузка всех существующих игр из базы данных один раз"""
        if not self.token:
            self.logger.error("Cannot load games: Not authenticated")
            raise Exception("Not authenticated")

        headers = {'Authorization': self.token}
        all_games = []
        page = 1
        per_page = 200

        self.logger.info("Starting to load existing games from API")
        try:
            while True:
                self.logger.debug(f"Fetching page {page} with {per_page} items per page")
                response = requests.get(
                    f'{self.base_url}/collections/games/records',
                    headers=headers,
                    params={'page': page, 'perPage': per_page}
                )
                response.raise_for_status()

                data = response.json()
                games_chunk = data.get('items', [])
                self.logger.debug(f"Received {len(games_chunk)} games on page {page}")
                all_games.extend(games_chunk)

                if len(games_chunk) < per_page:
                    self.logger.info(f"Finished loading games. Total pages: {page}")
                    break
                page += 1

            # Сохраняем игры в словарь с ключом по полю iframe_url
            for game in all_games:
                link = game.get('iframe_url', '')
                if link:
                    normalized_link = self.normalize_url(link)
                    self.existing_games[normalized_link] = game
                    self.logger.debug(f"Added game to dictionary: {normalized_link}")

            self.logger.info(f"Loaded {len(self.existing_games)} existing games into memory")
        except Exception as e:
            self.logger.error(f"Error loading existing games: {str(e)}")
            raise

    def game_exists(self, url):
        """
        Проверка, существует ли игра с заданной ссылкой

        Args:
            url: URL игры для проверки

        Returns:
            bool: True если игра уже существует, False если нет
        """
        normalized_url = self.normalize_url(url)
        self.logger.info(f"Checking if game exists with URL: {normalized_url}")
        exists = normalized_url in self.existing_games
        if exists:
            self.logger.info(f"Game with URL {normalized_url} already exists in database")
            self.logger.debug(f"Existing game data: {self.existing_games[normalized_url]}")
        else:
            self.logger.info(f"Game with URL {normalized_url} does not exist in database")
        return exists

    def normalize_url(self, url):
        """Нормализация URL для согласованности"""
        original_url = url
        url = url.rstrip('/')
        if url.endswith('/index.html'):
            url = url[:-len('/index.html')]
        self.logger.debug(f"Normalized URL: {original_url} -> {url}")
        return url

def main():
    # Тестовый запуск компонента
    logging.basicConfig(level=logging.DEBUG)  # Устанавливаем уровень DEBUG для подробных логов
    checker = GameChecker()
    if checker.login():
        checker.load_existing_games()
        test_url = "https://example.com/game/project.json"
        print(f"Game exists: {checker.game_exists(test_url)}")

if __name__ == "__main__":
    main()

==================================================
File: components/js_json_extractor.py
==================================================

# components/js_json_extractor.py

import json
import re
import logging
import time
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class JSJsonExtractor:
    """Extract JSON data from JavaScript files"""
    
    def __init__(self):
        self.driver = self._init_driver()
        self.json_pattern = re.compile(r'Store\(\{state:\{app:(.*?)\},getters:\{checkRequireds', re.DOTALL)
        
    def _init_driver(self):
        """Initialize Chrome WebDriver with performance logging"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})
        
        try:
            return webdriver.Chrome(
                service=Service(ChromeDriverManager().install()),
                options=options
            )
        except Exception as e:
            logger.error(f"Failed to initialize WebDriver: {str(e)}")
            raise

    def _extract_json(self, js_content):
        """Extract JSON content between markers"""
        try:
            match = self.json_pattern.search(js_content)
            if match:
                json_str = match.group(1).strip()
                if json_str.startswith('{') and json_str.endswith('}'):
                    return json.loads(json_str)
                elif json_str.startswith('//'):
                    # Handle commented JSON
                    json_str = json_str.split('\n', 1)[1]
                    return json.loads(json_str)
            return None
        except json.JSONDecodeError as e:
            logger.error(f"Failed to decode JSON: {str(e)}")
            return None

    def _json_to_md(self, data):
        """Convert JSON data to Markdown format"""
        md_content = []
        
        if isinstance(data, dict):
            if 'rows' in data:
                for row in data['rows']:
                    if 'titleText' in row:
                        md_content.append(f"## {row.get('title', '')}\n")
                        md_content.append(f"{row['titleText']}\n")
                    
                    if 'objects' in row:
                        for obj in row['objects']:
                            if 'title' in obj:
                                md_content.append(f"### {obj['title']}\n")
                            if 'text' in obj:
                                md_content.append(f"{obj['text']}\n")
            elif 'content' in data:
                md_content.append(f"# {data.get('title', 'Untitled')}\n")
                md_content.append(f"{data['content']}\n")
            elif 'sections' in data:
                for section in data['sections']:
                    md_content.append(f"## {section.get('title', '')}\n")
                    md_content.append(f"{section.get('text', '')}\n")
            else:
                for key, value in data.items():
                    if isinstance(value, str):
                        md_content.append(f"## {key}\n")
                        md_content.append(f"{value}\n")
        
        return "\n".join(md_content)

    def _capture_js_files(self, url):
        """Capture all loaded .js files"""
        try:
            self.driver.get(url)
            time.sleep(5)
            
            logs = self.driver.get_log('performance')
            js_urls = set()
            
            for log in logs:
                try:
                    message = json.loads(log['message'])
                    params = message['message']['params']
                    request = params['request']
                    
                    if 'url' in request and request['url'].endswith('.js'):
                        js_urls.add(request['url'])
                except (KeyError, json.JSONDecodeError):
                    continue
            
            return list(js_urls)
        except Exception as e:
            logger.error(f"Error capturing JS files: {str(e)}")
            return []

    def process_url(self, url):
        """Process single URL by extracting JSON from JS files"""
        try:
            js_urls = self._capture_js_files(url)
            
            if not js_urls:
                logger.warning(f"No JS files found for {url}")
                return None
            
            for js_url in js_urls:
                try:
                    response = requests.get(js_url)
                    if response.status_code == 200:
                        json_data = self._extract_json(response.text)
                        if json_data:
                            break
                except Exception as e:
                    logger.warning(f"Failed to process JS from {js_url}: {str(e)}")
                    continue
            else:
                logger.warning(f"No valid JSON data found in JS files for {url}")
                return None
            
            md_content = self._json_to_md(json_data)
            project_name = url.split('/')[-2]
            md_filename = f"markdown/{project_name}.md"
            game_title = url.split('/')[-2].replace('_', ' ')
            md_content = f"title: {game_title}\n\n{md_content}"
            
            with open(md_filename, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            logger.info(f"Successfully processed {url} to {md_filename}")
            return md_filename
            
        except Exception as e:
            logger.error(f"Error processing {url}: {str(e)}")
            return None

    def close(self):
        """Clean up resources"""
        try:
            if self.driver:
                self.driver.quit()
        except Exception as e:
            logger.error(f"Error closing WebDriver: {str(e)}")

def extract_js_json(url):
    """Convenience function for standalone usage"""
    extractor = JSJsonExtractor()
    try:
        return extractor.process_url(url)
    finally:
        extractor.close()

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: python js_json_extractor.py <url>")
        sys.exit(1)
        
    url = sys.argv[1]
    result = extract_js_json(url)
    if result:
        print(f"Successfully created: {result}")
    else:
        print("Failed to process URL")


==================================================
File: components/traffic_analyzer.py
==================================================

# components/traffic_analyzer.py

import json
import time
import logging
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TrafficAnalyzer:
    """Analyze network traffic to find and process game data files"""
    
    def __init__(self):
        self.driver = self._init_driver()
        
    def _init_driver(self):
        """Initialize Chrome WebDriver with performance logging"""
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--disable-gpu')
        options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})
        
        try:
            return webdriver.Chrome(
                service=Service(ChromeDriverManager().install()),
                options=options
            )
        except Exception as e:
            logger.error(f"Failed to initialize WebDriver: {str(e)}")
            raise

    def _json_to_md(self, data):
        """Convert JSON data to Markdown format with flexible structure handling"""
        md_content = []
        
        if isinstance(data, dict):
            if 'rows' in data:
                for row in data['rows']:
                    if 'titleText' in row:
                        md_content.append(f"## {row.get('title', '')}\n")
                        md_content.append(f"{row['titleText']}\n")
                    
                    if 'objects' in row:
                        for obj in row['objects']:
                            if 'title' in obj:
                                md_content.append(f"### {obj['title']}\n")
                            if 'text' in obj:
                                md_content.append(f"{obj['text']}\n")
            elif 'content' in data:
                md_content.append(f"# {data.get('title', 'Untitled')}\n")
                md_content.append(f"{data['content']}\n")
            elif 'sections' in data:
                for section in data['sections']:
                    md_content.append(f"## {section.get('title', '')}\n")
                    md_content.append(f"{section.get('text', '')}\n")
            else:
                for key, value in data.items():
                    if isinstance(value, str):
                        md_content.append(f"## {key}\n")
                        md_content.append(f"{value}\n")
        
        return "\n".join(md_content)

    def _capture_network_traffic(self, url):
        """Capture network traffic to find JSON files"""
        try:
            self.driver.get(url)
            time.sleep(5)
            
            logs = self.driver.get_log('performance')
            json_urls = set()
            
            for log in logs:
                try:
                    message = json.loads(log['message'])
                    params = message['message']['params']
                    request = params['request']
                    
                    if 'url' in request and request['url'].endswith('.json'):
                        json_urls.add(request['url'])
                except (KeyError, json.JSONDecodeError):
                    continue
            
            return list(json_urls)
        except Exception as e:
            logger.error(f"Error capturing network traffic: {str(e)}")
            return []

    def process_url(self, url):
        """Process single URL by analyzing network traffic"""
        try:
            json_urls = self._capture_network_traffic(url)
            
            if not json_urls:
                logger.warning(f"No JSON files found for {url}")
                return None
            
            for json_url in json_urls:
                try:
                    response = requests.get(json_url)
                    if response.status_code == 200:
                        data = response.json()
                        break
                except Exception as e:
                    logger.warning(f"Failed to process JSON from {json_url}: {str(e)}")
                    continue
            else:
                logger.warning(f"No valid data found in JSON files for {url}")
                return None
            
            md_content = self._json_to_md(data)
            project_name = url.split('/')[-2]
            md_filename = f"markdown/{project_name}.md"
            game_title = url.split('/')[-2].replace('_', ' ')
            md_content = f"title: {game_title}\n\n{md_content}"
            
            with open(md_filename, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            logger.info(f"Successfully processed {url} to {md_filename}")
            return md_filename
            
        except Exception as e:
            logger.error(f"Error processing {url}: {str(e)}")
            return None

    def close(self):
        """Clean up resources"""
        try:
            if self.driver:
                self.driver.quit()
        except Exception as e:
            logger.error(f"Error closing WebDriver: {str(e)}")

def analyze_traffic(url):
    """Convenience function for standalone usage"""
    analyzer = TrafficAnalyzer()
    try:
        return analyzer.process_url(url)
    finally:
        analyzer.close()

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: python traffic_analyzer.py <url>")
        sys.exit(1)
        
    url = sys.argv[1]
    result = analyze_traffic(url)
    if result:
        print(f"Successfully created: {result}")
    else:
        print("Failed to process URL")


==================================================
File: components/vector_search.py
==================================================

# components/vector_search.py

import os
import numpy as np
import argparse
import logging
import sys
from dotenv import load_dotenv
from openai import OpenAI
import chromadb

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - vector_search.py - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger()

# Load environment variables
load_dotenv()

# API configuration
API_KEY = os.getenv('DEEPINFRA_API_KEY')
MODEL_NAME = "BAAI/bge-en-icl"  # Default model
USE_M3_MODEL = False  # Flag for switching to M3 model

# Function to define the database path depending on the model
def get_chroma_dir(): 
    current_dir = os.path.dirname(os.path.abspath(__file__)) 
    base_dir = os.path.dirname(current_dir) 
    if USE_M3_MODEL:
        return os.path.join(base_dir, "chroma_db_BGE_M3")
    else:
        return os.path.join(base_dir, "chroma_db_BGE_EN_ICL")
    
if not API_KEY:
    logger.error("DEEPINFRA_API_KEY not found in environment variables or .env file")
    logger.error("Please set this value in your .env file or environment")
    sys.exit(1)

# Initialize DeepInfra client
client = OpenAI(
    api_key=API_KEY,
    base_url="https://api.deepinfra.com/v1/openai",
)

# Global variable for the collection
collection = None
chroma_client = None

def init_chroma_client():
    """Initialize ChromaDB client with the appropriate directory based on the model"""
    global chroma_client
    chroma_dir = get_chroma_dir()
    
    # Убедимся, что директория существует
    os.makedirs(chroma_dir, exist_ok=True)
    
    logger.info(f"Initializing ChromaDB client with directory: {chroma_dir}")
    chroma_client = chromadb.PersistentClient(path=chroma_dir)
    return chroma_client

def init_collection():
    """Initializes the collection with the correct distance settings"""
    global collection
    collection_name = "cyoa_games"  # Используем одно имя коллекции, т.к. они в разных директориях
    
    try:
        collection = chroma_client.get_collection(name=collection_name)
        count = collection.count()
        if count > 0:
            logger.info(f"Connected to existing ChromaDB collection '{collection_name}' with {count} documents")
        else:
            logger.warning(f"Collection '{collection_name}' exists but is empty")
    except Exception as e:
        logger.info(f"Creating new ChromaDB collection '{collection_name}': {str(e)}")
        collection = chroma_client.create_collection(
            name=collection_name,
            metadata={"hnsw:space": "cosine"}  
        )
    
    return collection

def reset_collection():
    """Resets the collection and creates a new one with the correct settings"""
    global collection
    collection_name = "cyoa_games"  # Используем одно имя коллекции
    
    try:
        # Try to get and delete the old collection
        chroma_client.delete_collection(collection_name)
        logger.info("Deleted existing collection")
    except Exception as e:
        logger.info(f"No existing collection to delete or error: {str(e)}")
    
    # Create a new collection with cosine distance
    collection = chroma_client.create_collection(
        name=collection_name, 
        metadata={"hnsw:space": "cosine"}   
    )
    logger.info(f"Created new collection '{collection_name}' with cosine distance")
    return collection

def generate_embeddings(text, is_query=False):
    """Generate embeddings for text using DeepInfra API"""
    logger.info(f"Generating embeddings for {'query' if is_query else 'document'} using {'BGE-M3' if USE_M3_MODEL else 'BGE-ICL'}")
    
    try:
        # Different handling for ICL vs M3 models
        if USE_M3_MODEL:
            # For M3 model, we don't use ICL examples
            if is_query:
                formatted_text = f"Represent this query for retrieving relevant CYOA games: {text}"
            else:
                formatted_text = f"Represent this CYOA game document for retrieval: {text}"
        else:
            # For ICL model, we use examples for queries
            if is_query:
                # Extensive and diverse examples in English for ICL
                icl_examples = """
Example 1:
Query: "CYOA with character development"
Relevant game: "An interactive story where the hero gains new abilities and develops skills after each choice, with branching storylines based on character growth"

Example 2:
Query: "horror elements CYOA"
Relevant game: "A dark adventure with frightening elements where choices affect character survival and mental stability in a supernatural setting"

Example 3:
Query: "post-apocalyptic mutants"
Relevant game: "CYOA story about survival in a radioactive world where the protagonist can mutate and gain new abilities depending on player choices"

Example 4:
Query: "fantasy multiple endings"
Relevant game: "A magical adventure with a branched narrative where player actions lead to one of 12 possible finales depending on key decisions"

Example 5:
Query: "dystopian cyberpunk"
Relevant game: "Neo-noir CYOA in a dark future with corporate control, where the hero makes choices between personal gain and resisting the system"

Example 6:
Query: "love triangle adventure"
Relevant game: "A story developing relationships between three characters amid a dangerous journey, where romantic choices impact the ending"

Example 7:
Query: "detective investigation"
Relevant game: "CYOA in noir genre with murder investigation where player collects clues and interrogates suspects through dialogue choices"

Example 8:
Query: "space exploration scifi"
Relevant game: "An interstellar adventure where the player makes critical decisions about alien encounters and ship management in the unknown regions of space"

Example 9:
Query: "medieval kingdom politics"
Relevant game: "A throne succession CYOA where court intrigue and political alliances determine your rise or fall as potential ruler"

Example 10:
Query: "time travel paradox"
Relevant game: "A complex narrative where players navigate temporal anomalies, with choices in one timeline affecting events in others"

Find for: "{}"
""".format(text)
                
                formatted_text = icl_examples
            else:
                # For documents in ICL model
                formatted_text = f"Represent this CYOA game for retrieval: {text}"
        
        # Get the current model name based on the flag
        current_model = "BAAI/bge-m3" if USE_M3_MODEL else MODEL_NAME
        
        response = client.embeddings.create(
            model=current_model,
            input=formatted_text,
            encoding_format="float"
        )
        
        # Extract the embedding from the response
        embedding = response.data[0].embedding
        
        logger.info(f"Embedding dimension: {len(embedding)}")
        logger.info(f"Used {response.usage.prompt_tokens} tokens for embedding")
        
        return embedding
        
    except Exception as e:
        logger.error(f"Error generating embeddings: {str(e)}")
        logger.error("IMPORTANT: Failed to get embeddings from the API. Check your API key and access rights.")
        return None

def process_single_file(file_path, url=None):
    """Process a single markdown file and return its embeddings and metadata"""
    logger.info(f"Processing file: {file_path}")
    
    try:
        # Check if the file exists
        if not os.path.exists(file_path):
            logger.error(f"File not found: {file_path}")
            return None, None
            
        with open(file_path, 'r', encoding='utf-8') as f:
            md_content = f.read()
        
        if not md_content.strip():
            logger.error(f"File is empty: {file_path}")
            return None, None
            
        logger.info(f"Content length: {len(md_content)} characters")
        
        embeddings = generate_embeddings(md_content, is_query=False)
        
        filename = os.path.basename(file_path)
        metadata = {
            'project': filename[:-3],  # Remove .md extension
            'file': filename,
            'url': url if url else "",
            'content': md_content[:1000]  # Save the beginning of the content for preview
        }
        logger.info(f"Successfully processed {file_path}")
        return embeddings, metadata
    except Exception as e:
        logger.error(f"Error processing {file_path}: {str(e)}")
        return None, None

def process_all_files():
    """Process all markdown files in summaries directory and add to ChromaDB"""
    md_dir = 'summaries'
    success_count = 0
    
    logger.info(f"Processing all files in {md_dir}")
    
    if not os.path.exists(md_dir):
        logger.error(f"Directory not found: {md_dir}")
        return False
    
    files = [f for f in os.listdir(md_dir) if f.endswith('.md')]
    logger.info(f"Found {len(files)} .md files")
    
    # Reset the collection before adding new documents
    reset_collection()
    
    for filename in files:
        file_path = os.path.join(md_dir, filename)
        embeddings, metadata = process_single_file(file_path)
        
        if embeddings is not None:
            try:
                collection.add(
                    embeddings=[embeddings],
                    metadatas=[metadata],
                    documents=[metadata.get('content', "")],
                    ids=[f"game_{filename[:-3]}"]  # Use the filename as ID
                )
                success_count += 1
                logger.info(f"Added {filename} to ChromaDB")
            except Exception as e:
                logger.error(f"Error adding {filename} to ChromaDB: {str(e)}")
    
    # После добавления всех документов, проверяем их количество
    count = collection.count()
    logger.info(f"Collection now has {count} documents")
    
    logger.info(f"Successfully processed {success_count} out of {len(files)} files")
    return success_count > 0

def update_database(file_path, url):
    """Update database with a single file"""
    logger.info(f"Updating database with file: {file_path}")
    
    # Process new file
    embeddings, metadata = process_single_file(file_path, url)
    if embeddings is None:
        logger.error("IMPORTANT: Failed to generate embeddings for the file. Database update aborted.")
        return False
    
    # Get the filename to create an ID
    filename = os.path.basename(file_path)
    game_id = f"game_{filename[:-3]}"
    
    try:
        # Check if an entry with this ID already exists
        existing = collection.get(ids=[game_id])
        if existing and 'ids' in existing and existing['ids']:
            # If it exists, delete the old entry
            logger.info(f"Entry with ID {game_id} already exists, updating...")
            collection.delete(ids=[game_id])
        else:
            logger.info(f"No existing entry with ID {game_id}")
        
        # Add the new entry
        collection.add(
            embeddings=[embeddings],
            metadatas=[metadata],
            documents=[metadata.get('content', "")],
            ids=[game_id]
        )
        
        # Проверяем количество документов после обновления
        count = collection.count()
        logger.info(f"Collection now has {count} documents after update")
        
        logger.info(f"Successfully updated database with {file_path}")
        return True
    except Exception as e:
        logger.error(f"Error updating ChromaDB: {str(e)}")
        return False

def search_similar_games(query_text, top_k=5):
    """Search for similar games based on a query"""
    logger.info(f"Searching for: '{query_text}' using {'BGE-M3' if USE_M3_MODEL else 'BGE-ICL'}")
    
    try:
        # Check if collection has data
        count = collection.count()
        if count == 0:
            logger.error("Collection is empty. Please initialize the database first.")
            return []
        
        # Generate embedding for the query
        query_embedding = generate_embeddings(query_text, is_query=True)
        
        if query_embedding is None:
            logger.error("Failed to generate embedding for query")
            return []
        
        # Perform the search in ChromaDB
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=min(top_k, count),  # Ensure we don't ask for more results than exist
            include=["metadatas", "distances"]
        )
        
        # Process the results
        found_games = []
        if results and results['metadatas']:
            for i, (metadata, distance) in enumerate(zip(results['metadatas'][0], results['distances'][0])):
                # В ChromaDB с косинусным расстоянием меньшие значения = большее сходство
                # Для удобства отображения конвертируем в значение сходства (1 - расстояние)
                similarity = 1 - distance  # Преобразуем расстояние в сходство
                
                found_games.append({
                    'project': metadata.get('project', 'Unknown'),
                    'file': metadata.get('file', 'Unknown'),
                    'url': metadata.get('url', ''),
                    'content_preview': metadata.get('content', '')[:200] + '...',
                    'similarity': float(similarity)  # Сходство: чем ближе к 1, тем более похожи
                })
        
        logger.info(f"Found {len(found_games)} similar games")
        return found_games
    except Exception as e:
        logger.error(f"Error searching: {str(e)}")
        return []

def debug_similarity(query, top_k=5):
    """Debug the similarity calculation between a query and documents in the collection"""
    logger.info(f"Debug similarity for query: '{query}' using {'BGE-M3' if USE_M3_MODEL else 'BGE-ICL'}")
    
    try:
        # Check if collection has data
        count = collection.count()
        if count == 0:
            logger.error("Collection is empty. Please initialize the database first.")
            return []
            
        # Get all documents in the collection
        all_data = collection.get()
        
        if not all_data or 'embeddings' not in all_data or not all_data['embeddings']:
            logger.error("No documents in collection")
            return []
        
        # Generate embedding for the query
        query_emb = generate_embeddings(query, is_query=True)
        
        if query_emb is None:
            logger.error("Failed to generate embedding for query")
            return []
        
        # Manually calculate cosine similarity
        similarities = []
        for i, doc_emb in enumerate(all_data["embeddings"]):
            # Normalize vectors for cosine similarity
            query_norm = np.linalg.norm(query_emb)
            doc_norm = np.linalg.norm(doc_emb)
            
            # Avoid division by zero
            if query_norm > 0 and doc_norm > 0:
                # Calculate cosine similarity
                cosine_sim = np.dot(query_emb, doc_emb) / (query_norm * doc_norm)
            else:
                cosine_sim = 0
                
            similarities.append((
                all_data["metadatas"][i].get("project", "Unknown"), 
                all_data["metadatas"][i].get("file", "Unknown"),
                all_data["metadatas"][i].get("url", ""),
                cosine_sim
            ))
        
        # Sort by similarity (highest first)
        similarities.sort(key=lambda x: x[3], reverse=True)
        
        # Convert to the same format as search results
        found_games = []
        for project, file, url, sim in similarities[:top_k]:
            found_games.append({
                'project': project,
                'file': file,
                'url': url,
                'content_preview': "...",  # Do not load content for debugging
                'similarity': sim
            })
        
        return found_games
    except Exception as e:
        logger.error(f"Error in debug similarity: {str(e)}")
        return []

def compare_embeddings(text1, text2):
    """Compare two texts directly and output their similarity"""
    logger.info(f"Comparing texts directly using {'BGE-M3' if USE_M3_MODEL else 'BGE-ICL'}")
    
    try:
        # Generate embeddings
        emb1 = generate_embeddings(text1, is_query=True)
        emb2 = generate_embeddings(text2, is_query=False)
        
        if emb1 is None or emb2 is None:
            logger.error("Failed to generate embeddings")
            return None
        
        # Calculate cosine similarity
        norm1 = np.linalg.norm(emb1)
        norm2 = np.linalg.norm(emb2)
        
        if norm1 > 0 and norm2 > 0:
            cosine_sim = np.dot(emb1, emb2) / (norm1 * norm2)
        else:
            cosine_sim = 0
        
        logger.info(f"Direct cosine similarity: {cosine_sim}")
        return cosine_sim
    except Exception as e:
        logger.error(f"Error comparing embeddings: {str(e)}")
        return None

def main():
    parser = argparse.ArgumentParser(description='Process markdown files for search database')
    parser.add_argument('--update', nargs=2, metavar=('FILE', 'URL'),
                       help='Update database with a single file and URL')
    parser.add_argument('--init', action='store_true',
                       help='Initialize database by processing all files in summaries folder')
    parser.add_argument('--search', type=str,
                       help='Search for similar games')
    parser.add_argument('--reset', action='store_true',
                      help='Reset the collection with proper distance metrics')
    parser.add_argument('--debug', type=str,
                      help='Debug similarity calculation with a query')
    parser.add_argument('--compare', nargs=2, metavar=('TEXT1', 'TEXT2'),
                      help='Compare similarity between two texts directly')
    parser.add_argument('-M3', action='store_true',
                      help='Use BAAI/bge-m3 model instead of BGE-ICL')
    parser.add_argument('--info', action='store_true',
                      help='Show information about the current database')
    
    args = parser.parse_args()
    
    # Set the global model flag if M3 is requested
    global USE_M3_MODEL
    if args.M3:
        USE_M3_MODEL = True
        logger.info("Using BAAI/bge-m3 model")
    
    # Инициализируем клиент ChromaDB с соответствующей директорией
    global chroma_client
    chroma_client = init_chroma_client()
    
    # Initialize the collection
    global collection
    collection = init_collection()
    
    # Check if collection has documents
    count = collection.count()
    logger.info(f"Collection has {count} documents")
    
    # Check API connection
    logger.info("Checking API connection...")
    try:
        # Get the current model name based on the flag
        current_model = "BAAI/bge-m3" if USE_M3_MODEL else MODEL_NAME
        
        test_response = client.embeddings.create(
            model=current_model,
            input="test",
            encoding_format="float"
        )
        logger.info("API connection successful")
        logger.info(f"Embedding dimension: {len(test_response.data[0].embedding)}")
    except Exception as e:
        logger.error(f"API connection error: {str(e)}")
        logger.error("Without a valid API key, embeddings cannot be generated correctly.")
        return
    
    # Process arguments
    if args.info:
        # Display information about the current database
        model_name = "BAAI/bge-m3" if USE_M3_MODEL else MODEL_NAME
        chroma_dir = get_chroma_dir()
        doc_count = collection.count()
        print(f"Database Information:")
        print(f"Model: {model_name}")
        print(f"Data directory: {chroma_dir}")
        print(f"Document count: {doc_count}")
        if doc_count == 0:
            print("Database is empty. Use --init to load data.")
    
    elif args.reset:
        reset_collection()
        collection = init_collection()  # Update the global variable
        print(f"Collection reset successfully for {'BGE-M3' if USE_M3_MODEL else 'BGE-ICL'}. Please re-add your data.")
    
    elif args.init:
        # Initialize database mode
        logger.info(f"Initializing database from all summaries using {'BGE-M3' if USE_M3_MODEL else 'BGE-ICL'}")
        if process_all_files():
            logger.info("Database initialized successfully with all summaries")
            print(f"Database initialized successfully with all summaries for {'BGE-M3' if USE_M3_MODEL else 'BGE-ICL'}")
        else:
            logger.error("Failed to initialize database")
            print("Failed to initialize database")
    
    elif args.update:
        # Update mode
        file_path, url = args.update
        logger.info(f"Update mode: processing {file_path} with URL {url}")
        if update_database(file_path, url):
            logger.info(f"Successfully updated database with {file_path}")
            print(f"Successfully updated database with {file_path}")
        else:
            logger.error(f"Failed to update database with {file_path}")
            print(f"Failed to update database with {file_path}")
    
    elif args.search:
        # Search mode
        query = args.search
        logger.info(f"Search mode: looking for '{query}'")
        
        # Check if collection has data
        count = collection.count()
        if count == 0:
            print("Database is empty. Please initialize the database first with --init")
            return
            
        results = search_similar_games(query, top_k=5)
        
        if results:
            print(f"\nSearch Results (using {'BGE-M3' if USE_M3_MODEL else 'BGE-ICL'}):")
            for i, result in enumerate(results, 1):
                print(f"\n{i}. {result['project']} (Similarity: {result['similarity']:.2f})")
                print(f"   URL: {result['url']}")
                print(f"   Preview: {result['content_preview']}")
        else:
            print("No results found")
    
    elif args.debug:
        # Debug mode
        query = args.debug
        logger.info(f"Debug mode: testing similarity for '{query}'")
        
        # Check if collection has data
        count = collection.count()
        if count == 0:
            print("Database is empty. Please initialize the database first with --init")
            return
            
        results = debug_similarity(query, top_k=5)
        
        if results:
            print(f"\nDebug Similarity Results (manual calculation using {'BGE-M3' if USE_M3_MODEL else 'BGE-ICL'}):")
            for i, result in enumerate(results, 1):
                print(f"\n{i}. {result['project']} (Similarity: {result['similarity']:.4f})")
                print(f"   URL: {result['url']}")
        else:
            print("No results for debug similarity")
    
    elif args.compare:
        # Compare mode
        text1, text2 = args.compare
        logger.info(f"Compare mode: testing similarity between two texts")
        similarity = compare_embeddings(text1, text2)
        
        if similarity is not None:
            print(f"\nDirect similarity between texts (using {'BGE-M3' if USE_M3_MODEL else 'BGE-ICL'}): {similarity:.4f}")
        else:
            print("Failed to compare texts")
    
    else:
        message = "Please specify a mode: --init to create new database, --update to add new entries, --search to find similar games, --reset to recreate the collection, --debug to test similarity, --compare to compare two texts directly, or --info to display database information. Use -M3 flag to switch to BAAI/bge-m3 model."
        logger.error(message)
        print(message)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        logger.critical(f"Unhandled exception: {str(e)}", exc_info=True)
        print(f"Critical error: {str(e)}")
        sys.exit(1)

==================================================
File: controller.py
==================================================

import subprocess
import sys
import datetime
import os
import time
import logging
import traceback
import asyncio
import concurrent.futures
import random
import json  # Добавлен импорт json
from components.traffic_analyzer import TrafficAnalyzer
from components.js_json_extractor import extract_js_json
from components.crawler import crawl_url, json_to_md
from components.game_checker import GameChecker
from components.project_downloader import crawl_and_download, create_session
from urllib.parse import urlparse

def setup_logging():
    os.makedirs("logs", exist_ok=True)
    date_str = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(f"logs/process_{date_str}.log"),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logging.info("Process started")
    
    return logging.getLogger()

def run_script(script_name, args=None):
    if not os.path.exists(script_name):
        logger.error(f"Script file not found: {script_name}")
        return False, "", f"File not found: {script_name}"
    
    try:
        if script_name.endswith('.js'):
            command = ['node', script_name]
        else:
            command = [sys.executable, script_name]
            
        if args:
            command.extend(args.split())
        
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        output_lines = []
        error_lines = []
        
        def process_output(stream, prefix, output_list):
            for line in stream:
                line = line.strip()
                if line:
                    output_list.append(line)
        
        process_output(process.stdout, "OUTPUT", output_lines)
        process_output(process.stderr, "ERROR", error_lines)
        
        process.wait()
        
        output = "\n".join(output_lines)
        error = "\n".join(error_lines)
        
        if process.returncode != 0:
            logger.error(f"Script returned non-zero exit code: {process.returncode}")
            return False, output, error
            
        return True, output, error
    
    except subprocess.TimeoutExpired:
        return False, "", "Timeout expired"
    except Exception as e:
        logger.error(f"Error running script: {script_name}")
        return False, "", str(e)

async def run_script_async(script_name, args=None, max_retries=3, retry_delay=5):
    attempt = 0
    last_error = ""
    
    while attempt < max_retries:
        attempt += 1
        
        try:
            with concurrent.futures.ThreadPoolExecutor() as pool:
                success, output, error = await asyncio.get_event_loop().run_in_executor(
                    pool, run_script, script_name, args
                )
                
                if success:
                    return success, output, error
                
                last_error = error
                
                if attempt < max_retries:
                    jitter = random.uniform(0.7, 1.3)
                    delay = retry_delay * jitter
                    await asyncio.sleep(delay)
        
        except Exception as e:
            last_error = str(e)
            
            if attempt < max_retries:
                jitter = random.uniform(0.7, 1.3)
                delay = retry_delay * jitter
                await asyncio.sleep(delay)
    
    logger.error(f"Script {script_name} failed after {max_retries} attempts. Last error: {last_error}")
    return False, "", last_error

def check_prerequisites():
    prerequisites_met = True
    
    for directory in ["markdown", "summaries", "screenshots"]:
        if not os.path.exists(directory):
            os.makedirs(directory, exist_ok=True)
    
    if not os.path.exists("links.txt"):
        logger.error("links.txt file not found!")
        prerequisites_met = False
    else:
        with open("links.txt", "r") as f:
            urls = [line.strip() for line in f if line.strip()]
            if not urls:
                logger.error("links.txt file is empty!")
                prerequisites_met = False
    
    try:
        result = subprocess.run(["node", "--version"], 
                               stdout=subprocess.PIPE, 
                               stderr=subprocess.PIPE, 
                               text=True)
        if result.returncode != 0:
            logger.error("Node.js not found! Required for JavaScript scripts.")
            prerequisites_met = False
    except:
        logger.error("Error checking Node.js. Make sure it's installed.")
        prerequisites_met = False
    
    return prerequisites_met

def normalize_url(url):
    url = url.rstrip('/')
    if url.endswith('/index.html'):
        url = url[:-len('/index.html')]
    return f"{url}/project.json"

async def main_async(force_screenshots=False):
    MAX_CONCURRENT_SCREENSHOTS = 5
    MAX_RETRIES = 3
    
    if not check_prerequisites():
        logger.error("Prerequisites check failed. Please fix the issues above.")
        return
    
    game_checker = GameChecker()
    if not game_checker.login():
        logger.error("Failed to authenticate with GameChecker. Aborting.")
        return
    game_checker.load_existing_games()
    
    logger.info("Starting main processing loop")
    
    failed_urls = []
    visual_analysis_failures = []
    processed_urls = []
    skipped_urls = []
    newly_processed_urls = []
    
    # Создаем сессию для скачивания
    download_session = create_session()
    
    # Путь для сохранения скачанных игр
    downloaded_games_dir = "downloaded_games"
    os.makedirs(downloaded_games_dir, exist_ok=True)
    
    try:
        with open('links.txt', 'r') as f:
            urls = [line.strip() for line in f if line.strip()]
        logger.info(f"Loaded {len(urls)} URLs from links.txt")
    except Exception as e:
        logger.error(f"Error reading links.txt: {str(e)}")
        return
    
    screenshot_semaphore = asyncio.Semaphore(MAX_CONCURRENT_SCREENSHOTS)
    
    async def create_screenshot(base_url, project_name):
        async with screenshot_semaphore:
            webp_path = f"screenshots/{project_name}.webp"
            
            if not force_screenshots and os.path.exists(webp_path):
                file_size = os.path.getsize(webp_path)
                if file_size > 0:
                    logger.info(f"Using existing screenshot: {webp_path}")
                    return True, "Existing screenshot used", ""
                else:
                    logger.warning(f"Found empty screenshot file: {webp_path}, regenerating")
            
            logger.info(f"Generating new screenshot for {base_url}")
            success, output, error = await run_script_async("get_screenshoot_puppy.js", base_url, max_retries=MAX_RETRIES)
            if success and os.path.exists(webp_path):
                logger.info(f"Screenshot successfully created: {webp_path}")
            else:
                logger.error(f"Screenshot generation failed: {error}")
            return success, output, error
    
    for index, url in enumerate(urls, 1):
        logger.info(f"Processing URL {index}/{len(urls)}: {url}")
        
        # Получаем имя домена и игры для структуры папок
        parsed_url = urlparse(url)
        domain = parsed_url.netloc
        path_parts = parsed_url.path.strip('/').split('/')
        game_name = path_parts[-1] if path_parts else ''
        download_path = os.path.join(downloaded_games_dir, f"{domain}/{game_name}")
        
        # Скачиваем игру целиком независимо от того, есть она в каталоге или нет 
        logger.info(f"Downloading game to {download_path}")
        completed, downloaded, failed = crawl_and_download(
            url,
            download_path,
            session=download_session,
            max_workers=5
        )
        logger.info(f"Download result: Processed {completed} files, Downloaded {downloaded}, Failed {failed}")
            
        # Проверяем наличие игры в каталоге после скачивания
        if game_checker.game_exists(url):
            logger.info(f"Skipping URL {url} as it already exists in the catalog")
            skipped_urls.append(url)
            processed_urls.append(url)
            continue
        
        try:
            project_json_url = normalize_url(url)
            project_name = url.split('/')[-2]
            md_file = f"{project_name}.md"
            md_path = f"markdown/{md_file}"
            
            # Проверяем наличие локального project.json
            local_json_path = os.path.join(download_path, "project.json")
            if os.path.exists(local_json_path):
                logger.info(f"Using local project.json from {local_json_path}")
                with open(local_json_path, 'r', encoding='utf-8') as f:
                    json_data = json.load(f)
                md_content = json_to_md(json_data)
                os.makedirs("markdown", exist_ok=True)
                with open(md_path, 'w', encoding='utf-8') as f:
                    game_title = project_name.replace('_', ' ')
                    game_url = url
                    f.write(f"Game URL: {game_url}\n\nPossible title: {game_title}\n\n{md_content}")
            else:
                # Если локального JSON нет (хотя он должен быть после скачивания)
                logger.warning(f"Local project.json not found at {local_json_path}, falling back to network methods")
                if crawl_url(project_json_url):
                    logger.info(f"Text extracted via crawl_url for {url}")
                else:
                    text_content = None
                    result = extract_js_json(url)
                    if result:
                        text_content = result
                        logger.info(f"Text extracted via extract_js_json for {url}")
                    else:
                        analyzer = TrafficAnalyzer()
                        try:
                            result = analyzer.process_url(url)
                            if result:
                                text_content = result
                                logger.info(f"Text extracted via TrafficAnalyzer for {url}")
                        finally:
                            analyzer.close()
                    
                    if text_content:
                        os.makedirs("markdown", exist_ok=True)
                        with open(md_path, 'w', encoding='utf-8') as f:
                            f.write(text_content)
                    else:
                        logger.error(f"All processing methods failed for URL: {url}")
                        failed_urls.append(url)
                        continue
            
            processed_urls.append(url)
            newly_processed_urls.append(url)
            
            base_url = normalize_url(url).replace('/project.json', '/')
            
            success, output, error = await create_screenshot(base_url, project_name)
            webp_path = f"screenshots/{project_name}.webp"
            if not success:
                logger.error(f"Screenshot processing failed for {base_url}: {error}")
                visual_analysis_failures.append(base_url)
            elif not os.path.exists(webp_path):
                logger.error(f"Screenshot file not found after processing: {webp_path}")
                visual_analysis_failures.append(base_url)
            
            logger.info(f"Running summarization for {md_file} in sent_search mode")
            success, output, error = await run_script_async(
                "summarize.py", 
                f"{md_file} --mode sent_search", 
                max_retries=MAX_RETRIES
            )
            if not success:
                logger.error(f"Summarization failed for {url}: {error}")
             
            logger.info("Pausing for 0.5s before next summarization...")
            await asyncio.sleep(0.5)

            logger.info(f"Running summarization for {md_file} in catalog mode")
            success, output, error = await run_script_async(
                "summarize.py", 
                f"{md_file} --mode catalog", 
                max_retries=MAX_RETRIES
            )
            if not success:
                logger.error(f"Catalog summarization failed for {url}: {error}")
        
        except Exception as e:
            logger.error(f"Unhandled exception processing URL {url}: {str(e)}")
            failed_urls.append(url)
    
    # Закрываем сессию скачивания
    download_session.close()
    
    # Запуск prepare_and_upload.py только если есть новые обработанные URL
    if newly_processed_urls:
        test_mode = '--test' in sys.argv
        logger.info(f"Running prepare_and_upload.py in {'test' if test_mode else 'live'} mode")
        upload_args = " --test" if test_mode else ""
        success, output, error = await run_script_async(
            "prepare_and_upload.py",
            upload_args,
            max_retries=MAX_RETRIES
        )
        if not success:
            logger.error(f"prepare_and_upload.py failed: {error}")
        else:
            logger.info("prepare_and_upload.py completed successfully")
    else:
        logger.info("No new URLs processed. Skipping prepare_and_upload.py execution.")
    
    # Генерация отчёта
    logger.info("Generating final report")
    timestamp = datetime.datetime.now().strftime('[%Y-%m-%d %H:%M:%S]')
    
    report = [
        f"\n{timestamp} Processing summary:",
        f"  Total URLs processed: {len(urls)}",
        f"  Successfully processed: {len(processed_urls)}",
        f"  Newly processed: {len(newly_processed_urls)}",
        f"  Skipped (already in catalog): {len(skipped_urls)}",
        f"  Failed to process: {len(failed_urls)}",
        f"  Visual analysis failures: {len(visual_analysis_failures)}"
    ]
    
    if failed_urls:
        report.append("  Failed URLs:")
        for url in failed_urls:
            report.append(f"    - {url}")
            
    if visual_analysis_failures:
        report.append("  Visual analysis failed URLs (VPN required):")
        for url in visual_analysis_failures:
            report.append(f"    - {url}")
    
    if skipped_urls:
        report.append("  Skipped URLs (already in catalog):")
        for url in skipped_urls:
            report.append(f"    - {url}")
    
    if newly_processed_urls:
        report.append("  Newly processed URLs:")
        for url in newly_processed_urls:
            report.append(f"    - {url}")
    
    for line in report:
        logger.info(line)
    
    with open('log.txt', 'a') as log_file:
        for line in report:
            log_file.write(f"{line}\n")
    
    print("\n=== Processing Report ===")
    for line in report:
        print(line)
    
    logger.info("Process completed")

def main():
    force_screenshots = '--force-screenshots' in sys.argv
    asyncio.run(main_async(force_screenshots))

if __name__ == "__main__":
    logger = setup_logging()
    
    try:
        main()
    except Exception as e:
        logger.critical("Unhandled exception in main process")
        logger.critical(str(e))
        logger.critical(traceback.format_exc())
        print("\nCRITICAL ERROR: Process failed. See logs for details.")
        sys.exit(1)

==================================================
File: GameUploader.py
==================================================

# GameUploader.py

import os
import json
import glob
from dotenv import load_dotenv
import requests
from pathlib import Path
import mimetypes
import sys
import shutil
import time
import logging

load_dotenv()

# Настройка логирования
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("logs/game_uploader.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

ALLOWED_IMAGE_MIME_TYPES = [
    'image/png', 'image/jpeg', 'image/gif', 'image/webp', 'image/avif', 'image/svg+xml'
]

EXTENSION_TO_MIME = {
    '.png': 'image/png', '.jpg': 'image/jpeg', '.jpeg': 'image/jpeg',
    '.gif': 'image/gif', '.webp': 'image/webp', '.avif': 'image/avif', '.svg': 'image/svg+xml'
}

class AuthorManager:
    def __init__(self, base_url, token):
        self.base_url = base_url
        self.token = token
        self.authors_cache = {}
        logger.info("AuthorManager initialized")

    def load_authors(self):
        logger.info("Loading existing authors from API")
        try:
            if not self.token:
                raise Exception("Not authenticated")
            headers = {'Authorization': self.token}
            all_authors = []
            page = 1
            per_page = 200
            while True:
                response = requests.get(
                    f'{self.base_url}/collections/authors/records',
                    headers=headers,
                    params={'page': page, 'perPage': per_page}
                )
                response.raise_for_status()
                data = response.json()
                authors_chunk = data.get('items', [])
                all_authors.extend(authors_chunk)
                if len(authors_chunk) < per_page:
                    break
                page += 1
            logger.info(f'Downloaded {len(all_authors)} existing authors')
            for author in all_authors:
                self.authors_cache[author['name'].lower()] = author['id']
            return all_authors
        except Exception as e:
            logger.error(f'Error getting existing authors: {e}')
            return []

    def create_author(self, name, description=""):
        logger.info(f"Creating new author: {name}")
        try:
            if not self.token:
                raise Exception("Not authenticated")
            headers = {'Authorization': self.token}
            response = requests.post(
                f'{self.base_url}/collections/authors/records',
                headers=headers,
                json={'name': name, 'description': description}
            )
            response.raise_for_status()
            author_data = response.json()
            logger.info(f'Created author: {name} with ID: {author_data["id"]}')
            self.authors_cache[name.lower()] = author_data["id"]
            return author_data["id"]
        except Exception as e:
            logger.error(f'Error creating author "{name}": {e}')
            return None

    def get_or_create_author(self, name, description=""):
        # Поддержка списка авторов: обрабатываем только одного автора здесь
        if isinstance(name, list):
            logger.warning("get_or_create_author received a list; use get_or_create_authors for multiple authors")
            if not name:
                return None
            name = name[0]  # Для обратной совместимости берём первого
        name_lower = name.lower()
        if name_lower in self.authors_cache:
            logger.info(f"Found existing author: {name} (ID: {self.authors_cache[name_lower]})")
            return self.authors_cache[name_lower]
        return self.create_author(name, description)

    def get_or_create_authors(self, authors, description=""):
        """Обрабатывает список авторов и возвращает их ID."""
        if not isinstance(authors, list):
            authors = [authors]  # Преобразуем строку в список из одного элемента
        author_ids = []
        for name in authors:
            if not name:
                logger.warning("Empty author name encountered, skipping")
                continue
            name_lower = name.lower()
            if name_lower in self.authors_cache:
                logger.info(f"Found existing author: {name} (ID: {self.authors_cache[name_lower]})")
                author_ids.append(self.authors_cache[name_lower])
            else:
                author_id = self.create_author(name, description)
                if author_id:
                    author_ids.append(author_id)
        return author_ids

class TagManager:
    def __init__(self):
        self.base_url = 'https://cyoa.cafe/api'
        self.email = os.getenv('EMAIL')
        self.password = os.getenv('PASSWORD')
        self.token = None
        self.category_id = "phc2n4pqe7hxe36"
        self.existing_tags = {}
        logger.info("TagManager initialized")

    def login(self):
        logger.info("Attempting TagManager login")
        try:
            response = requests.post(
                f'{self.base_url}/collections/users/auth-with-password',
                json={'identity': self.email, 'password': self.password}
            )
            response.raise_for_status()
            data = response.json()
            self.token = data['token']
            logger.info('TagManager successfully logged in')
            return True
        except Exception as e:
            logger.error(f'TagManager login error: {e}')
            return False

    def get_all_tags(self):
        logger.info("Loading all existing tags")
        try:
            if not self.token:
                raise Exception("Not authenticated")
            headers = {'Authorization': self.token}
            all_tags = []
            page = 1
            per_page = 200
            while True:
                response = requests.get(
                    f'{self.base_url}/collections/tags/records',
                    headers=headers,
                    params={'page': page, 'perPage': per_page}
                )
                response.raise_for_status()
                data = response.json()
                tags_chunk = data.get('items', [])
                all_tags.extend(tags_chunk)
                if len(tags_chunk) < per_page:
                    break
                page += 1
            logger.info(f'Downloaded {len(all_tags)} existing tags')
            self.existing_tags = {}
            for tag in all_tags:
                tag_name_lower = tag['name'].lower()
                self.existing_tags[tag_name_lower] = {
                    'id': tag['id'], 'name': tag['name'], 'description': tag.get('description', '')
                }
            return all_tags
        except Exception as e:
            logger.error(f'Error getting existing tags: {e}')
            return []

    def create_tag(self, name, description=""):
        logger.info(f"Creating new tag: {name}")
        try:
            if not self.token:
                raise Exception("Not authenticated")
            headers = {'Authorization': self.token}
            response = requests.post(
                f'{self.base_url}/collections/tags/records',
                headers=headers,
                json={'name': name, 'description': description}
            )
            response.raise_for_status()
            tag_data = response.json()
            logger.info(f'Created tag: {name} with ID: {tag_data["id"]}')
            self.existing_tags[name.lower()] = {
                'id': tag_data["id"], 'name': name, 'description': description
            }
            self.add_tag_to_category(tag_data["id"])
            return tag_data["id"]
        except Exception as e:
            logger.error(f'Error creating tag "{name}": {e}')
            return None

    def add_tag_to_category(self, tag_id):
        logger.info(f"Adding tag {tag_id} to category Custom")
        try:
            if not self.token:
                raise Exception("Not authenticated")
            headers = {'Authorization': self.token}
            response = requests.get(
                f'{self.base_url}/collections/tag_categories/records/{self.category_id}',
                headers=headers
            )
            response.raise_for_status()
            category_data = response.json()
            current_tags = category_data.get('tags', [])
            if tag_id not in current_tags:
                current_tags.append(tag_id)
                response = requests.patch(
                    f'{self.base_url}/collections/tag_categories/records/{self.category_id}',
                    headers=headers,
                    json={'tags': current_tags}
                )
                response.raise_for_status()
                logger.info(f'Added tag {tag_id} to category Custom')
                return True
            else:
                logger.info(f'Tag {tag_id} already in category Custom')
                return True
        except Exception as e:
            logger.error(f'Error adding tag {tag_id} to category: {e}')
            return False

    def get_or_create_tag(self, tag_name):
        tag_name_lower = tag_name.lower()
        if tag_name_lower in self.existing_tags:
            logger.info(f"Found existing tag: {tag_name} (ID: {self.existing_tags[tag_name_lower]['id']})")
            return self.existing_tags[tag_name_lower]['id']
        return self.create_tag(tag_name)

class GameUploader:
    def __init__(self):
        self.base_url = 'https://cyoa.cafe/api'
        self.email = os.getenv('EMAIL')
        self.password = os.getenv('PASSWORD')
        self.token = None
        self.tag_manager = TagManager()
        self.author_manager = None
        self.request_delay = 3
        logger.info("GameUploader initialized")

    def login(self):
        logger.info("Attempting to login")
        try:
            response = requests.post(
                f'{self.base_url}/collections/users/auth-with-password',
                json={'identity': self.email, 'password': self.password}
            )
            response.raise_for_status()
            data = response.json()
            self.token = data['token']
            logger.info("Successfully logged in")
            self.tag_manager.login()
            self.tag_manager.get_all_tags()
            logger.info(f"Loaded {len(self.tag_manager.existing_tags)} tags into cache")
            self.author_manager = AuthorManager(self.base_url, self.token)
            self.author_manager.load_authors()
            logger.info(f"Loaded {len(self.author_manager.authors_cache)} authors into cache")
            return data
        except Exception as e:
            logger.error(f'Login failed: {str(e)}')
            return None

    def create_game(self, game_data):
        game_data['img_or_link'] = game_data['img_or_link'].lower()
        logger.info(f"Creating game: {game_data['title']}")
        try:
            if not self.token:
                raise Exception("Not authenticated")
            headers = {'Authorization': self.token}
            image_path = Path(game_data['image'])
            if not image_path.exists():
                raise FileNotFoundError(f"Cover image not found: {image_path}")

            file_ext = image_path.suffix.lower()
            mime_type = EXTENSION_TO_MIME.get(file_ext, mimetypes.guess_type(str(image_path))[0])
            if mime_type not in ALLOWED_IMAGE_MIME_TYPES:
                raise ValueError(f"Unsupported image format: {mime_type}")

            tag_ids = []
            if game_data.get('tags'):
                for tag_name in game_data['tags']:
                    tag_id = self.tag_manager.get_or_create_tag(tag_name)
                    if tag_id:
                        tag_ids.append(tag_id)
                    else:
                        logger.warning(f"Failed to get or create tag '{tag_name}'")

            # Получаем список ID авторов
            author_ids = []
            if 'author' in game_data:
                author_ids = self.author_manager.get_or_create_authors(game_data['author'])
                logger.info(f"Authors for game: {game_data['author']} (IDs: {author_ids})")

            form_data = [
                ('title', game_data['title']),
                ('description', game_data['description']),
                ('img_or_link', game_data['img_or_link']),
                ('uploader', game_data['uploader']),
            ]
            if game_data['img_or_link'] == 'link' and game_data.get('iframe_url'):
                form_data.append(('iframe_url', game_data['iframe_url']))
            for tag_id in tag_ids:
                form_data.append(('tags', tag_id))
            # Добавляем всех авторов в form_data
            for author_id in author_ids:
                form_data.append(('authors', author_id))

            files = {}
            with open(image_path, 'rb') as cover_image_file:
                files['image'] = ('blob', cover_image_file, mime_type)
                if game_data['img_or_link'] == 'img' and game_data.get('cyoa_pages'):
                    for i, page_path in enumerate(game_data['cyoa_pages']):
                        page_path_obj = Path(page_path)
                        if page_path_obj.exists():
                            page_mime_type = EXTENSION_TO_MIME.get(page_path_obj.suffix.lower(), mimetypes.guess_type(str(page_path_obj))[0])
                            if page_mime_type not in ALLOWED_IMAGE_MIME_TYPES:
                                logger.warning(f"Unsupported format for page {page_path}: {page_mime_type}")
                                continue
                            with open(page_path_obj, 'rb') as page_file:
                                files[f'cyoa_pages[{i}]'] = (f'page_{i}', page_file, page_mime_type)
                        else:
                            logger.warning(f"CYOA page not found: {page_path}")

                logger.debug(f"Form data: {form_data}")
                logger.debug(f"Files: {[k for k in files.keys()]}")
                response = requests.post(
                    f'{self.base_url}/collections/games/records',
                    headers=headers,
                    data=form_data,
                    files=files
                )
                logger.info(f"API response status: {response.status_code}")
                logger.debug(f"API response text: {response.text}")
                response.raise_for_status()
                game_record = response.json()
                logger.info(f"Game created successfully: {game_record['id']}")

            # Связываем авторов с игрой (на случай, если API не обработал authors в form_data)
            for author_id in author_ids:
                time.sleep(self.request_delay)
                self.link_game_to_author(game_record['id'], author_id)

            return game_record
        except Exception as e:
            logger.error(f"Failed to create game '{game_data['title']}': {str(e)}", exc_info=True)
            raise

    def link_game_to_author(self, game_id, author_id):
        logger.info(f"Linking game {game_id} to author {author_id}")
        try:
            if not self.token:
                raise Exception("Not authenticated")
            headers = {'Authorization': self.token}
            # Получаем текущие данные игры
            response = requests.get(
                f'{self.base_url}/collections/games/records/{game_id}',
                headers=headers
            )
            response.raise_for_status()
            game_data = response.json()
            current_authors = game_data.get('authors', [])
            if author_id not in current_authors:
                current_authors.append(author_id)
                response = requests.patch(
                    f'{self.base_url}/collections/games/records/{game_id}',
                    headers=headers,
                    json={'authors': current_authors}
                )
                response.raise_for_status()
                logger.info(f'Linked game {game_id} to author {author_id}')
            else:
                logger.info(f'Game {game_id} already linked to author {author_id}')
            # Обновляем автора (двусторонняя связь)
            response = requests.get(
                f'{self.base_url}/collections/authors/records/{author_id}',
                headers=headers
            )
            response.raise_for_status()
            author_data = response.json()
            current_games = author_data.get('games', [])
            if game_id not in current_games:
                current_games.append(game_id)
                response = requests.patch(
                    f'{self.base_url}/collections/authors/records/{author_id}',
                    headers=headers,
                    json={'games': current_games}
                )
                response.raise_for_status()
                logger.info(f'Updated author {author_id} with game {game_id}')
            return True
        except Exception as e:
            logger.error(f'Error linking game {game_id} to author {author_id}: {e}')
            return False

def move_processed_files(game_data, processed_folder):
    logger.info(f"Moving processed files for {game_data['title']} to {processed_folder}")
    try:
        os.makedirs(processed_folder, exist_ok=True)
        logger.info(f"Ensured folder exists: {processed_folder}")
        json_path = game_data.get('json_path')
        if not json_path or not os.path.exists(json_path):
            json_path = None
            for file in os.listdir("New_Games"):
                if file.endswith(".json"):
                    candidate_path = os.path.join("New_Games", file)
                    if os.path.getsize(candidate_path) == 0:
                        logger.warning(f"Found empty file {file}")
                        continue
                    try:
                        with open(candidate_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            if data.get('title') == game_data['title']:
                                json_path = candidate_path
                                break
                    except json.JSONDecodeError as e:
                        logger.error(f"Error decoding JSON in {candidate_path}: {e}")
                        continue
        if not json_path:
            logger.warning(f"Could not find JSON file for game {game_data['title']}")
            return False
        json_filename = os.path.basename(json_path)
        base_name = os.path.splitext(json_filename)[0]
        shutil.move(json_path, os.path.join(processed_folder, json_filename))
        logger.info(f"Moved JSON file: {json_filename}")
        cover_path = game_data['image']
        cover_filename = os.path.basename(cover_path)
        shutil.move(cover_path, os.path.join(processed_folder, cover_filename))
        logger.info(f"Moved cover image: {cover_filename}")
        if game_data['img_or_link'] == 'img' and game_data.get('cyoa_pages'):
            pages_folder = os.path.join("New_Games", base_name)
            if os.path.isdir(pages_folder):
                processed_pages_folder = os.path.join(processed_folder, base_name)
                os.makedirs(processed_pages_folder, exist_ok=True)
                for page_file in os.listdir(pages_folder):
                    page_path = os.path.join(pages_folder, page_file)
                    if os.path.isfile(page_path):
                        shutil.move(page_path, os.path.join(processed_pages_folder, page_file))
                if len(os.listdir(pages_folder)) == 0:
                    os.rmdir(pages_folder)
                logger.info(f"Moved CYOA pages folder: {base_name}")
        return True
    except Exception as e:
        logger.error(f"Error moving processed files: {e}")
        return False

def load_games_from_folder(folder_path):
    logger.info(f"Loading games from folder: {folder_path}")
    games = []
    json_files = glob.glob(os.path.join(folder_path, "*.json"))
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                game_data = json.load(f)
            base_name = os.path.splitext(os.path.basename(json_file))[0]
            image_path = None
            for ext in EXTENSION_TO_MIME.keys():
                img_path = os.path.join(folder_path, f"{base_name}{ext}")
                if os.path.exists(img_path):
                    image_path = img_path
                    break
            if not image_path:
                logger.warning(f"Cover image not found for {json_file}")
                continue
            game_data['image'] = image_path
            game_data['json_path'] = json_file
            if 'img_or_link' not in game_data:
                game_data['img_or_link'] = "img"
            if game_data['img_or_link'] == 'link' and 'iframe_url' not in game_data:
                logger.warning(f"iframe_url missing for link-type game in {json_file}")
                continue
            if game_data['img_or_link'] == 'img':
                if 'cyoa_pages' not in game_data or not game_data['cyoa_pages']:
                    pages_folder = os.path.join(folder_path, base_name)
                    if os.path.isdir(pages_folder):
                        page_files = []
                        for ext in EXTENSION_TO_MIME.keys():
                            page_files.extend(glob.glob(os.path.join(pages_folder, f"*{ext}")))
                        page_files.sort()
                        if page_files:
                            game_data['cyoa_pages'] = page_files
                        else:
                            logger.warning(f"No CYOA pages found in folder {pages_folder}")
                    else:
                        logger.warning(f"CYOA pages folder {pages_folder} not found")
                if 'cyoa_pages' not in game_data or not game_data['cyoa_pages']:
                    logger.warning(f"No CYOA pages specified for img-type game in {json_file}")
                    continue
            if 'uploader' not in game_data:
                game_data['uploader'] = "mar1q123caruaaw"
            games.append(game_data)
            logger.info(f"Loaded game: {game_data['title']} ({game_data['img_or_link']})")
        except Exception as e:
            logger.error(f"Error loading {json_file}: {e}")
    return games

def main():
    uploader = GameUploader()
    processed_folder = "Processed_Games"
    try:
        auth_data = uploader.login()
        if not auth_data:
            raise Exception("Failed to login")
        games = load_games_from_folder("New_Games")
        logger.info(f"Found {len(games)} games to upload")
        for i, game_data in enumerate(games):
            try:
                logger.info(f"Uploading game {i+1}/{len(games)}: {game_data['title']}")
                record = uploader.create_game(game_data)
                logger.info(f"Successfully uploaded: {game_data['title']} (ID: {record['id']})")
                if move_processed_files(game_data, processed_folder):
                    logger.info(f"Files for {game_data['title']} moved to {processed_folder}")
                if i < len(games) - 1:
                    logger.info(f"Waiting {uploader.request_delay} seconds before next upload")
                    time.sleep(uploader.request_delay)
            except Exception as e:
                logger.error(f"Failed to upload {game_data.get('title', 'Unknown')}: {str(e)}")
    except Exception as e:
        logger.critical(f"Critical error in main: {str(e)}", exc_info=True)

if __name__ == "__main__":
    main()

==================================================
File: get_screenshoot_puppy.js
==================================================

// get_screenshoot_puppy.js

// to run use  "   node get_screenshoot_puppy.js https://example.neocities.org/game/ --pause   " 

const puppeteer = require('puppeteer');
const sharp = require('sharp');
const fs = require('fs');

(async () => {
  // Parse command line arguments
  const args = process.argv.slice(2);
  const url = args[0];
  const startPaused = args.includes('--pause');
  const windowSizeArg = args.find(arg => arg.startsWith('--window-size='));
  let windowWidth = 2420; // Default value
  let windowHeight = 1420; // Default value

  if (windowSizeArg) {
    const [width, height] = windowSizeArg.split('=')[1].split(',').map(Number);
    windowWidth = width || windowWidth;
    windowHeight = height || windowHeight;
  }

  if (!url) {
    console.error('URL argument is required');
    process.exit(1);
  }

  // Launch the browser with the specified window size
  const browser = await puppeteer.launch({
    args: [
      '--no-sandbox',
      `--window-size=${windowWidth},${windowHeight}`
    ],
    headless: false,
    protocolTimeout: 300000
  });
  const page = await browser.newPage();

  const viewportWidth = 1920;
  const screenshotHeight = 2560;
  await page.setViewport({ 
    width: viewportWidth,
    height: 1080, // Initial height for interaction
    deviceScaleFactor: 2
  });

  console.log('Navigating to:', url);
  await page.goto(url, {
    waitUntil: 'networkidle0',
    timeout: 300000
  });

  await page.waitForSelector('body', { timeout: 300000 });
  console.log('Page URL after navigation:', page.url());

  // Force enable scrolling
  await page.evaluate(() => {
    document.body.style.overflow = 'auto';
    document.documentElement.style.overflow = 'auto';
    document.body.style.height = 'auto';
  });

  // Add virtual menu
  await addControlMenu(page);

  // Wait for "STOP" and "CONTINUE" presses
  let isPaused = startPaused;
  if (startPaused) {
    console.log('Started in paused mode. Adjust the page manually, then press "CONTINUE".');
  }

  page.on('console', msg => {
    if (msg.text() === 'STOP_PRESSED') {
      isPaused = true;
      console.log('Paused. Adjust the page manually, then press "CONTINUE".');
    } else if (msg.text() === 'CONTINUE_PRESSED') {
      isPaused = false;
      console.log('Continuing screenshot process...');
    }
  });

  // Wait until the user presses "STOP" and "CONTINUE"
  let attempts = 0;
  const maxAttempts = 300;
  while (attempts < maxAttempts) {
    if (isPaused) {
      await new Promise(resolve => setTimeout(resolve, 1000));
      continue;
    }

    if (!isPaused && (attempts > 0 || startPaused)) {
      break;
    }

    await new Promise(resolve => setTimeout(resolve, 1000));
    attempts++;
  }

  if (attempts >= maxAttempts) {
    throw new Error('Timeout waiting for user interaction');
  }

  // Remove the menu before taking the screenshot
  await page.evaluate(() => {
    const menu = document.getElementById('control-menu');
    if (menu) menu.remove();
  });

  // Set full size for the screenshot
  await page.setViewport({
    width: viewportWidth,
    height: screenshotHeight,
    deviceScaleFactor: 2
  });

  // Take the screenshot
  const screenshotName = generateScreenshotName(page.url());
  const screenshotPath = `screenshots/${screenshotName}.png`;
  const webpPath = `screenshots/${screenshotName}.webp`;

  await page.screenshot({ 
    path: screenshotPath,
    clip: { x: 0, y: 0, width: viewportWidth, height: screenshotHeight }
  });

  // Convert to WebP
  await sharp(screenshotPath)
    .resize({
      width: Math.round(viewportWidth * 0.5),
      height: Math.round(screenshotHeight * 0.5),
      fit: 'inside',
      withoutEnlargement: true
    })
    .webp({ quality: 80 })
    .toFile(webpPath);

  console.log(`Screenshot saved: ${webpPath}`);

  // Clean up temporary file
  fs.unlinkSync(screenshotPath);
  await browser.close();
})();

// Function to add virtual menu
async function addControlMenu(page) {
  await page.evaluate(() => {
    const menu = document.createElement('div');
    menu.id = 'control-menu';
    menu.style.position = 'absolute';
    menu.style.top = '10px';
    menu.style.left = '10px';
    menu.style.zIndex = '9999';
    menu.style.background = 'rgba(0, 0, 0, 0.8)';
    menu.style.padding = '20px';
    menu.style.borderRadius = '10px';
    menu.style.display = 'flex';
    menu.style.gap = '20px';

    const stopButton = document.createElement('button');
    stopButton.innerText = 'STOP'; // Translated from "СТОП"
    stopButton.style.padding = '15px 30px';
    stopButton.style.fontSize = '24px';
    stopButton.style.background = '#ff4444';
    stopButton.style.color = 'white';
    stopButton.style.border = 'none';
    stopButton.style.cursor = 'pointer';
    stopButton.style.borderRadius = '8px';
    stopButton.onclick = () => console.log('STOP_PRESSED');

    const continueButton = document.createElement('button');
    continueButton.innerText = 'CONTINUE'; // Translated from "ПРОДОЛЖИТЬ"
    continueButton.style.padding = '15px 30px';
    continueButton.style.fontSize = '24px';
    continueButton.style.background = '#44ff44';
    continueButton.style.color = 'white';
    continueButton.style.border = 'none';
    continueButton.style.cursor = 'pointer';
    continueButton.style.borderRadius = '8px';
    continueButton.onclick = () => console.log('CONTINUE_PRESSED');

    menu.appendChild(stopButton);
    menu.appendChild(continueButton);
    document.body.appendChild(menu);
  });
}

// Generate filename from URL
function generateScreenshotName(url) {
  const parsedUrl = new URL(url);
  const pathParts = parsedUrl.pathname.split('/').filter(Boolean);
  let screenshotName;

  if (pathParts.length > 0) {
    if (pathParts[pathParts.length - 1] === 'index.html') {
      screenshotName = pathParts[pathParts.length - 2] || parsedUrl.hostname.split('.')[0];
    } else {
      screenshotName = pathParts[pathParts.length - 1];
    }
  } else {
    screenshotName = parsedUrl.hostname;
  }
  return screenshotName;
}

==================================================
File: prepare_and_upload.py
==================================================

# prepare_and_upload.py

import os
import shutil
import subprocess
import sys
import json
import re
from pathlib import Path
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/prepare_and_upload.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Constants
CATALOG_JSON_DIR = "catalog_json"
SCREENSHOTS_DIR = "screenshots"
NEW_GAMES_DIR = "New_Games"
PROCESSED_GAMES_DIR = "Processed_Games"
GAME_UPLOADER_SCRIPT = "GameUploader.py"

def remove_json_comments(json_text):
    """Remove single-line comments (//) from JSON, preserving them in strings."""
    lines = json_text.splitlines()
    cleaned_lines = []
    in_string = False
    i = 0

    while i < len(lines):
        line = lines[i]
        cleaned_line = ""
        j = 0

        while j < len(line):
            if line[j] == '"' and (j == 0 or line[j-1] != '\\'):
                in_string = not in_string
                cleaned_line += line[j]
                j += 1
            elif not in_string and j + 1 < len(line) and line[j:j+2] == '//':
                break
            else:
                cleaned_line += line[j]
                j += 1

        cleaned_line = cleaned_line.rstrip()
        if cleaned_line:
            cleaned_lines.append(cleaned_line)
        i += 1

    return '\n'.join(cleaned_lines)

def strip_markdown_wrappers(json_text):
    """Strip Markdown code block wrappers (e.g., ```json and ```) from JSON text."""
    lines = json_text.splitlines()
    cleaned_lines = []
    
    # Skip lines that are part of Markdown code block markers
    for line in lines:
        stripped_line = line.strip()
        if stripped_line == "```json" or stripped_line == "```":
            continue
        cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

def validate_and_clean_json(json_path):
    """Validate and clean JSON from comments and Markdown wrappers, return original and cleaned text."""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        # First strip Markdown wrappers
        content_no_markdown = strip_markdown_wrappers(original_content)
        # Then remove comments
        cleaned_content = remove_json_comments(content_no_markdown)
        
        # Validate the cleaned content
        try:
            json.loads(cleaned_content)
            logger.info(f"JSON validated successfully after cleaning: {json_path}")
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in {json_path} after cleaning: {e}")
            logger.debug(f"Cleaned content:\n{cleaned_content}")
            return None, None
        
        return original_content, cleaned_content
    except Exception as e:
        logger.error(f"Error processing JSON {json_path}: {e}")
        return None, None

def prepare_game_files(test_mode=False):
    """Copy JSON and screenshot to New_Games for each game, skipping invalid files."""
    if not os.path.exists(CATALOG_JSON_DIR):
        logger.error(f"Directory not found: {CATALOG_JSON_DIR}")
        return False

    os.makedirs(NEW_GAMES_DIR, exist_ok=True)
    json_files = [f for f in os.listdir(CATALOG_JSON_DIR) if f.endswith(".json")]
    if not json_files:
        logger.warning(f"No JSON files found in {CATALOG_JSON_DIR}")
        return False

    logger.info(f"Found {len(json_files)} JSON files to process")
    success_count = 0
    for json_file in json_files:
        try:
            project_name = os.path.splitext(json_file)[0]
            json_src = os.path.join(CATALOG_JSON_DIR, json_file)
            screenshot_src = os.path.join(SCREENSHOTS_DIR, f"{project_name}.webp")
            json_dest = os.path.join(NEW_GAMES_DIR, json_file)
            json_with_comments_dest = os.path.join(NEW_GAMES_DIR, f"{project_name}_with_comments.json")
            screenshot_dest = os.path.join(NEW_GAMES_DIR, f"{project_name}.webp")

            original_json, cleaned_json = validate_and_clean_json(json_src)
            if cleaned_json is None:
                logger.warning(f"Skipping {json_file} due to invalid JSON after cleaning")
                continue

            with open(json_dest, 'w', encoding='utf-8') as f:
                f.write(cleaned_json)
            logger.info(f"Copied cleaned JSON: {json_file} to {NEW_GAMES_DIR}")

            with open(json_with_comments_dest, 'w', encoding='utf-8') as f:
                f.write(original_json)
            logger.info(f"Copied JSON with comments: {project_name}_with_comments.json to {NEW_GAMES_DIR}")

            if os.path.exists(screenshot_src):
                shutil.copy2(screenshot_src, screenshot_dest)
                logger.info(f"Copied screenshot: {project_name}.webp to {NEW_GAMES_DIR}")
            else:
                logger.warning(f"Screenshot not found: {screenshot_src}")

            success_count += 1
        except Exception as e:
            logger.error(f"Error preparing files for {json_file}: {e}")
            continue  # Continue processing other files

    if success_count == 0:
        logger.error("No files were successfully prepared")
        return False
    logger.info(f"Successfully prepared {success_count} out of {len(json_files)} files")
    return True

def run_game_uploader():
    """Run GameUploader.py."""
    if not os.path.exists(GAME_UPLOADER_SCRIPT):
        logger.error(f"Script not found: {GAME_UPLOADER_SCRIPT}")
        return False

    try:
        logger.info("Starting GameUploader.py")
        result = subprocess.run(
            [sys.executable, GAME_UPLOADER_SCRIPT],
            check=True,
            text=True,
            capture_output=True
        )
        logger.info("GameUploader.py completed successfully")
        logger.debug(f"Output:\n{result.stdout}")
        if result.stderr:
            logger.warning(f"GameUploader.py stderr:\n{result.stderr}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"GameUploader.py failed with exit code {e.returncode}: {e.output}")
        logger.debug(f"Stderr: {e.stderr}")
        return False
    except Exception as e:
        logger.error(f"Unexpected error running GameUploader.py: {e}")
        return False

def cleanup_catalog_json():
    """Remove all files from catalog_json after successful processing."""
    if os.path.exists(CATALOG_JSON_DIR):
        for file in os.listdir(CATALOG_JSON_DIR):
            file_path = os.path.join(CATALOG_JSON_DIR, file)
            try:
                if os.path.isfile(file_path):
                    os.remove(file_path)
                    logger.info(f"Removed {file} from {CATALOG_JSON_DIR}")
            except Exception as e:
                logger.error(f"Error removing {file_path}: {e}")

def move_comments_files_to_processed():
    """Move files with comments to Processed_Games."""
    os.makedirs(PROCESSED_GAMES_DIR, exist_ok=True)
    for file in os.listdir(NEW_GAMES_DIR):
        if file.endswith("_with_comments.json"):
            src = os.path.join(NEW_GAMES_DIR, file)
            dest = os.path.join(PROCESSED_GAMES_DIR, file)
            shutil.move(src, dest)
            logger.info(f"Moved {file} to {PROCESSED_GAMES_DIR}")

def main():
    test_mode = "--test" in sys.argv
    logger.info(f"Running in {'test' if test_mode else 'live'} mode")

    if not prepare_game_files(test_mode):
        logger.error("Failed to prepare game files. Aborting.")
        sys.exit(1)

    if not test_mode:
        logger.info("Starting game upload process")
        if not run_game_uploader():
            logger.error("GameUploader failed. Aborting.")
            sys.exit(1)
        logger.info("Game uploading completed successfully")
        
        move_comments_files_to_processed()
        cleanup_catalog_json()
    else:
        logger.info("Test mode: Skipping upload, file moving, and cleanup.")

if __name__ == "__main__":
    main()

==================================================
File: summarize.py
==================================================

# summarize.py

import os
import asyncio
import sys
import json
import subprocess
import pandas as pd
import logging
from fuzzywuzzy import fuzz
from urllib.parse import urlparse, unquote
from components.grok3_api import GrokAPI

# Настройка логирования
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/summarize.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Константы
SENT_SEARCH_PROMPT_PATH = "prompts/Grok_for_sent_search.md"
CATALOG_PROMPT_PATH = "prompts/Grok_description_for_catalog.md"
SUMMARIES_DIR = "summaries"
CATALOG_JSON_DIR = "catalog_json"
MARKDOWN_DIR = "markdown"
CSV_PATH = "games.csv"

def load_prompt(prompt_path):
    logger.info(f"Loading prompt from {prompt_path}")
    if not os.path.exists(prompt_path):
        raise FileNotFoundError(f"Prompt file not found: {prompt_path}")
    with open(prompt_path, 'r', encoding='utf-8') as f:
        return f.read()

def load_game_text(md_path):
    logger.info(f"Loading game text from {md_path}")
    if not os.path.exists(md_path):
        raise FileNotFoundError(f"Game text file not found: {md_path}")
    with open(md_path, 'r', encoding='utf-8') as f:
        return f.read()

def get_authors_list():
    logger.info("Fetching authors list")
    try:
        result = subprocess.run(
            ["python", "components/api_authors.py"], 
            capture_output=True, 
            text=True, 
            check=True
        )
        return result.stdout.strip()
    except Exception as e:
        logger.error(f"Error getting authors list: {str(e)}")
        return ""

def get_tag_categories():
    logger.info("Fetching tag categories")
    try:
        result = subprocess.run(
            ["python", "components/api_tags.py"], 
            capture_output=True, 
            text=True, 
            check=True
        )
        return result.stdout.strip()
    except Exception as e:
        logger.error(f"Error getting tag categories: {str(e)}")
        return ""

async def run_vision_query(webp_path, max_retries=3):
    logger.info(f"Running vision query for {webp_path}")
    try:
        from controller import run_script_async
        success, output, error = await run_script_async("vision_query.py", webp_path, max_retries=max_retries)
        if success and output and not output.startswith("Visual analysis error:"):
            logger.info("Vision query successful")
            return output.strip()
        logger.error(f"Vision analysis failed: {error}")
        return None
    except Exception as e:
        logger.error(f"Error in vision query: {str(e)}")
        return None

def get_csv_hint(project_name):
    logger.info(f"Getting CSV hint for {project_name}")
    if not os.path.exists(CSV_PATH):
        logger.warning(f"CSV file not found at: {CSV_PATH}")
        return "\n\n=== CSV Hint ===\nCSV file not found."
    try:
        df = pd.read_csv(CSV_PATH, encoding='utf-8')
        required_columns = ['Title', 'Author', 'Type', 'Static', 'Interactive']
        if not all(col in df.columns for col in required_columns):
            missing = [col for col in required_columns if col not in df.columns]
            logger.warning(f"CSV is missing required columns: {missing}")
            return "\n\n=== CSV Hint ===\nCSV is missing required columns."
        project_name_normalized = unquote(project_name.lower()).replace(" ", "")
        matches = []
        for index, row in df.iterrows():
            csv_title = str(row['Title']) if pd.notna(row['Title']) else ""
            csv_url = str(row['Static']) if pd.notna(row['Static']) else ""
            csv_interactive = str(row['Interactive']) if pd.notna(row['Interactive']) else ""
            csv_title_normalized = csv_title.lower().replace(" ", "")
            url_similarity = 0
            if csv_url and csv_url != "nan":
                url_normalized = unquote(csv_url.lower()).replace(" ", "")
                url_path = urlparse(csv_url).path.rstrip('/').split('/')[-1].lower()
                url_path_normalized = unquote(url_path).replace(" ", "")
                url_similarity = max(
                    fuzz.ratio(project_name_normalized, url_normalized),
                    fuzz.ratio(project_name_normalized, url_path_normalized)
                )
            interactive_similarity = 0
            if csv_interactive and csv_interactive != "nan":
                interactive_normalized = unquote(csv_interactive.lower()).replace(" ", "")
                interactive_similarity = fuzz.ratio(project_name_normalized, interactive_normalized)
            title_similarity = fuzz.ratio(project_name_normalized, csv_title_normalized)
            max_similarity = max(title_similarity, url_similarity, interactive_similarity)
            if max_similarity >= 70:
                matches.append((row, max_similarity))
        if not matches:
            return "\n\n=== CSV Hint ===\nNo matching entries found in CSV for this project name."
        matches.sort(key=lambda x: x[1], reverse=True)
        hint = "\n\n=== CSV Hint ===\nPossible matches from CSV based on project name:\n"
        for match, similarity in matches[:3]:
            hint += (
                f"- Title: {match.get('Title', 'N/A')}, "
                f"Author: {match.get('Author', 'N/A')}, "
                f"Type: {match.get('Type', 'N/A')} (Similarity: {similarity}%)\n"
            )
        hint += (
            "\nNote: When specifying the author, use the exact name as listed above (this is their nickname). "
            "The 'Type' (SFW or NSFW) is based on visual assessment. SFW strongly indicates the absence of nudity, "
            "while NSFW suggests mature content. Rely on this classification for consistency.\n"
        )
        return hint
    except Exception as e:
        logger.error(f"Unexpected error processing CSV: {str(e)}")
        return "\n\n=== CSV Hint ===\nUnexpected error processing CSV."

async def summarize_md_file(md_file, grok_api, mode="sent_search"):
    logger.info(f"Starting summarization for {md_file} in {mode} mode")
    project_name = os.path.splitext(md_file)[0]
    md_path = os.path.join(MARKDOWN_DIR, md_file)
    webp_path = f"screenshots/{project_name}.webp"
    
    if mode == "sent_search":
        prompt_path = SENT_SEARCH_PROMPT_PATH
        output_dir = SUMMARIES_DIR
        output_path = os.path.join(output_dir, f"{project_name}.md")
    elif mode == "catalog":
        prompt_path = CATALOG_PROMPT_PATH
        output_dir = CATALOG_JSON_DIR
        output_path = os.path.join(output_dir, f"{project_name}.json")
    else:
        logger.error(f"Unknown mode: {mode}")
        raise ValueError(f"Unknown mode: {mode}")

    try:
        prompt = load_prompt(prompt_path)
        game_text = load_game_text(md_path)
    except Exception as e:
        logger.error(f"Error loading files for {md_file}: {str(e)}")
        return False

    vision_description = ""
    if os.path.exists(webp_path):
        vision_output = await run_vision_query(webp_path)
        if vision_output:
            vision_description = (
                "\n\n=== Screenshot Description (First Page) ===\n"
                f"{vision_output}\n"
                "This describes the visual style and content of the game's first page, "
                "which can be used to enhance the game's description."
            )

    additional_data = ""
    if mode == "catalog":
        logger.info("Fetching additional data for catalog mode")
        authors_list = get_authors_list()
        tag_categories = get_tag_categories()
        if authors_list:
            additional_data += f"\n\n=== Available Authors ===\n{authors_list}\n"
        if tag_categories:
            additional_data += f"\n\n=== Available Tag Categories ===\n{tag_categories}\n"
        csv_hint = get_csv_hint(project_name)
        additional_data += csv_hint

    full_prompt = f"{prompt}{additional_data}\n\n=== Game Text ===\n{game_text}{vision_description}"
    logger.info(f"Sending prompt to Grok API for {md_file}")
    
    try:
        response = grok_api.ask(full_prompt, timeout=120)
        if response.startswith("Error:"):
            logger.error(f"Grok 3 API failed for {md_file}: {response}")
            return False
    except Exception as e:
        logger.error(f"Exception during Grok API call: {str(e)}")
        return False

    logger.info(f"API response received: {response[:100]}...")
    os.makedirs(output_dir, exist_ok=True)
    if mode == "sent_search":
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(response)
        logger.info(f"Summary saved to {output_path}")
    elif mode == "catalog":
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(response)
            logger.info(f"Catalog JSON saved to {output_path}")
        except Exception as e:
            logger.error(f"Error saving JSON for {md_file}: {e}")
            return False

    return True

async def main():
    if len(sys.argv) < 2:
        logger.error("No markdown file provided. Usage: python summarize.py <markdown_file> [--mode sent_search|catalog]")
        sys.exit(1)

    md_file = sys.argv[1]
    mode = "sent_search"
    if len(sys.argv) > 2 and sys.argv[2] == "--mode":
        if len(sys.argv) > 3:
            mode = sys.argv[3]
            if mode not in ["sent_search", "catalog"]:
                logger.error(f"Invalid mode: {mode}. Use 'sent_search' or 'catalog'")
                sys.exit(1)
        else:
            logger.error("Error: --mode requires a value (sent_search or catalog)")
            sys.exit(1)

    logger.info(f"Initializing Grok API")
    try:
        grok_api = GrokAPI(reuse_window=True, anonymous_chat=True)
    except Exception as e:
        logger.error(f"Failed to initialize Grok API: {str(e)}")
        sys.exit(1)

    logger.info(f"Processing {md_file} in {mode} mode...")
    success = await summarize_md_file(md_file, grok_api, mode=mode)
    if not success:
        logger.error(f"Failed to process {md_file}")

if __name__ == "__main__":
    asyncio.run(main())

==================================================
File: components/project_downloader.py
==================================================

# components/project_downloader.py

import os
import re
import json
import logging
from urllib.parse import urljoin, urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
from time import sleep
from bs4 import BeautifulSoup
from functools import lru_cache
from pathlib import Path
from email.utils import formatdate
from time import time
import chardet
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import threading
from pathlib import Path
import requests
import logging

# Set up logging
log_file_path = os.path.join('logs', 'project_downloader.log')
logging.basicConfig(
    filename=log_file_path,
    level=logging.DEBUG,  # Установим DEBUG для более детального анализа
    format='%(asctime)s - %(levelname)s - %(message)s'
)

metadata_lock = threading.Lock()

# -------------------- Helper Functions -------------------- #

def detect_encoding(content):
    logging.debug("Detecting encoding for content.")
    result = chardet.detect(content)
    return result['encoding'] if result['encoding'] else 'utf-8'

@lru_cache(maxsize=1000)
def is_valid_url(url, base_domain):
    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        return False
    if parsed.scheme not in {'http', 'https'}:
        return False
    if parsed.netloc != base_domain:
        return False
    if re.search(r'[()<>{}\s\\]', parsed.path):
        return False
    return True

def extract_urls_from_css(css_content):
    urls = re.findall(r'url\((?:\'|"|)(.*?)(?:\'|"|)\)', css_content)
    return urls

def is_local_resource(src, base_url):
    if src.startswith('http://') or src.startswith('https://'):
        return urlparse(src).netloc == urlparse(base_url).netloc
    if src.startswith('//'):
        return urlparse(base_url).netloc == src.split('/')[2]
    return True

def sanitize_folder_name(name):
    return re.sub(r'[<>:"/\\|?*]', '_', name)

def get_game_name(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    path = parsed_url.path.strip('/')
    if not path:
        return domain, ''
    path_parts = path.split('/')
    game_name = path_parts[-1] if path_parts else ''
    return domain, game_name

def enumerate_project_resources(data, directories=['images', 'music', 'videos', 'fonts', 'css', 'js', 'audio', 'assets']):
    if isinstance(data, dict):
        for key, value in data.items():
            if isinstance(value, str):
                for directory in directories:
                    if value.startswith(f"{directory}/"):
                        yield value
            elif isinstance(value, (dict, list)):
                yield from enumerate_project_resources(value, directories)
    elif isinstance(data, list):
        for item in data:
            yield from enumerate_project_resources(item, directories)

# -------------------- Downloading Function -------------------- #

def create_session():
    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(
        max_retries=retry_strategy,
        pool_connections=100,
        pool_maxsize=100,
        pool_block=False
    )
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': '*/*',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive'
    })
    return session

def download_file(url, path, session, base_domain, metadata_path, retries=3, delay=5, request_delay=0.1):
    """
    Скачивает файл и обновляет метаданные, полагаясь только на ETag для проверки актуальности.
    
    Args:
        url (str): URL файла для загрузки
        path (str): Локальный путь для сохранения файла
        session (requests.Session): Сессия для HTTP-запросов
        base_domain (str): Базовый домен для проверки валидности URL
        metadata_path (str): Путь к файлу метаданных (metadata.json)
        retries (int): Количество попыток повторного запроса
        delay (int): Задержка между повторными попытками
        request_delay (float): Задержка после успешной загрузки
    
    Returns:
        tuple: (success: bool, was_downloaded: bool)
    """
    path = Path(path)
    metadata_path = Path(metadata_path)

    # Пропускаем специальные случаи
    if url.endswith('favicon.ico') or url.startswith('data:'):
        logging.debug(f"Skipping special URL: {url}")
        return True, False

    # Читаем существующие метаданные
    metadata = {}
    if metadata_path.exists():
        try:
            with metadata_path.open('r', encoding='utf-8') as f:
                metadata = json.load(f)
        except Exception as e:
            logging.warning(f"Could not load metadata from {metadata_path}: {e}")

    # Проверка актуальности по ETag
    if path.exists() and url in metadata:
        local_metadata = metadata.get(url, {})
        local_etag = local_metadata.get('ETag')
        logging.debug(f"Checking file: {url}, Local ETag: {local_etag}, File exists: {path.exists()}")

        if local_etag:
            try:
                headers = {'If-None-Match': local_etag}
                head = session.head(url, allow_redirects=True, timeout=15, headers=headers)
                head.raise_for_status()

                if head.status_code == 304:
                    logging.debug(f"File up to date (304 Not Modified): {path}")
                    return True, False

                server_etag = head.headers.get('ETag')
                if server_etag == local_etag:
                    logging.debug(f"File matches by ETag: {path}")
                    return True, False
            except requests.RequestException as e:
                logging.warning(f"HEAD request failed for {url}: {e}, proceeding to download")

    # Скачивание файла
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        with session.get(url, stream=True, timeout=15) as response:
            response.raise_for_status()
            content_type = response.headers.get('Content-Type', '')
            server_etag = response.headers.get('ETag')

            is_text_file = (
                path.suffix.lower() in {'.html', '.htm', '.js', '.css', '.json', '.txt', '.xml', '.svg'} or
                'text' in content_type or 'javascript' in content_type
            )
            if is_text_file:
                content = response.content
                encoding = detect_encoding(content)
                text = content.decode(encoding, errors='replace')
                path.write_text(text, encoding='utf-8')
            else:
                with path.open('wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)

            # Обновляем метаданные с синхронизацией
            with metadata_lock:
                if metadata_path.exists():
                    try:
                        with metadata_path.open('r', encoding='utf-8') as f:
                            metadata = json.load(f)
                    except Exception:
                        logging.warning(f"Reloading metadata failed, using empty dict: {metadata_path}")
                        metadata = {}
                metadata[url] = {'ETag': server_etag}
                metadata_path.parent.mkdir(parents=True, exist_ok=True)
                with metadata_path.open('w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2)

            logging.debug(f"Downloaded and updated metadata: {url} -> {path}, Server ETag: {server_etag}")
            sleep(request_delay)
            return True, True
    except Exception as e:
        logging.error(f"Failed to download {url}: {e}")
        return False, False

def parse_html_for_resources(html_content, base_url, base_domain):
    soup = BeautifulSoup(html_content, 'html.parser')
    resources = set()
    parsed_base = urlparse(base_url)
    if not base_url.endswith('/'):
        base_url += '/'

    tags = soup.find_all(['link', 'script', 'img', 'video', 'audio', 'source'])
    logging.debug(f"Found {len(tags)} tags with potential resources")

    for tag in tags:
        src = tag.get('href') or tag.get('src')
        if src:
            logging.debug(f"Processing resource: {src} from tag: {tag.name}")
            src = src.replace('\\', '/').strip()
            if is_local_resource(src, base_url):
                if not src.startswith('data:'):
                    if src.startswith('/'):
                        full_url = f"{parsed_base.scheme}://{parsed_base.netloc}{src}"
                    else:
                        full_url = urljoin(base_url, src)
                    if is_valid_url(full_url, base_domain):
                        logging.debug(f"Adding resource: {full_url}")
                        resources.add(full_url)

    for style_tag in soup.find_all('style'):
        css_content = style_tag.string
        if css_content:
            urls = extract_urls_from_css(css_content)
            for url in urls:
                url = url.replace('\\', '/').strip()
                if url.startswith('/'):
                    full_url = f"{parsed_base.scheme}://{parsed_base.netloc}{url}"
                else:
                    full_url = urljoin(base_url, url)
                if is_valid_url(full_url, base_domain):
                    resources.add(full_url)

    for tag in soup.find_all(style=True):
        style_content = tag['style']
        urls = extract_urls_from_css(style_content)
        for url in urls:
            url = url.replace('\\', '/').strip()
            if url.startswith('/'):
                full_url = f"{parsed_base.scheme}://{parsed_base.netloc}{url}"
            else:
                full_url = urljoin(base_url, url)
            if is_valid_url(full_url, base_domain):
                resources.add(full_url)

    embedded_scripts = soup.find_all('script')
    for script in embedded_scripts:
        if script.string:
            js_urls = re.findall(r"""['"]([^'"]+?\.js(?:\?.*)?)['"]""", script.string)
            for js_url in js_urls:
                js_url = js_url.replace('\\', '/').strip()
                if is_local_resource(js_url, base_url):
                    if js_url.startswith('/'):
                        full_js_url = f"{parsed_base.scheme}://{parsed_base.netloc}{js_url}"
                    else:
                        full_js_url = urljoin(base_url, js_url)
                    if is_valid_url(full_js_url, base_domain):
                        resources.add(full_js_url)

    logging.debug(f"Total resources found: {len(resources)}")
    return resources

def parse_css_for_resources(css_content, base_url, base_domain):
    resources = set()
    urls = extract_urls_from_css(css_content)
    for url in urls:
        url = url.replace('\\', '/').strip()
        full_url = urljoin(base_url, url)
        if is_valid_url(full_url, base_domain):
            resources.add(full_url)
    return resources

def handle_resource(full_url, session, base_path, base_url_path, base_domain, metadata_path):
    logging.debug(f"Starting to handle resource: {full_url}")
    parsed_url = urlparse(full_url)
    path = parsed_url.path.lstrip('/')
    base_url_path_clean = base_url_path.lstrip('/').rstrip('/')
    if path.startswith(base_url_path_clean):
        relative_path = path[len(base_url_path_clean):].lstrip('/')
    else:
        relative_path = path
    file_path = os.path.join(base_path, relative_path)
    logging.debug(f"Saving resource to: '{file_path}'")
    if not is_valid_url(full_url, base_domain):
        logging.warning(f"Invalid or external URL skipped: {full_url}")
        return False
    success, was_downloaded = download_file(full_url, file_path, session, base_domain, metadata_path)
    if not success:
        logging.error(f"Failed to download resource: {full_url}")
        return False
    if file_path.endswith('.css'):
        try:
            with open(file_path, 'r', encoding='utf-8') as css_file:
                css_content = css_file.read()
            css_resources = parse_css_for_resources(css_content, full_url, base_domain)
            logging.debug(f"Found {len(css_resources)} resources in CSS: {file_path}")
            for css_res in css_resources:
                handle_resource(css_res, session, base_path, base_url_path, base_domain, metadata_path)
        except Exception as e:
            logging.error(f"Error parsing CSS {file_path}: {e}")
    return success

def crawl_and_download(url, base_path, session=None, max_workers=5):
    if session is None:
        session = create_session()
    
    base_path = Path(base_path)
    metadata_path = base_path / 'metadata.json'

    # Загрузка index.html
    index_path = base_path / 'index.html'
    index_success, index_downloaded = download_file(
        url, index_path, session, urlparse(url).netloc, metadata_path
    )
    if not index_success:
        logging.error(f"Failed to download index.html for {url}")
        return 0, 0, 0

    raw_content = index_path.read_bytes() if index_path.exists() else b""
    encoding = detect_encoding(raw_content)
    try:
        html_content = raw_content.decode(encoding, errors='replace')
    except Exception as e:
        logging.error(f"Error decoding content from {url}: {e}")
        html_content = ""

    parsed_base = urlparse(url)
    base_domain = parsed_base.netloc
    base_url_path = parsed_base.path
    if not base_url_path.endswith('/'):
        base_url_path += '/'

    resources = parse_html_for_resources(
        html_content, 
        f"{parsed_base.scheme}://{parsed_base.netloc}{parsed_base.path}", 
        base_domain
    )

    project_json_url = urljoin(f"{parsed_base.scheme}://{base_domain}{base_url_path}", 'project.json')
    project_json_path = base_path / 'project.json'
    project_json_success, project_json_downloaded = download_file(
        project_json_url, 
        project_json_path, 
        session, 
        base_domain,
        metadata_path,
        retries=3,
        delay=1
    )

    if project_json_success and project_json_downloaded:
        logging.debug(f"Successfully downloaded project.json from {project_json_url}")
        try:
            project_data = json.loads(project_json_path.read_text(encoding='utf-8'))
            project_resources = enumerate_project_resources(project_data)
            for res in project_resources:
                full_res_url = urljoin(f"{parsed_base.scheme}://{base_domain}{base_url_path}", res)
                if is_valid_url(full_res_url, base_domain):
                    resources.add(full_res_url)
                    logging.debug(f"Added resource from project.json: {full_res_url}")
        except json.JSONDecodeError:
            logging.error(f"Error decoding project.json from {project_json_url}")
        except Exception as e:
            logging.error(f"Unexpected error processing project.json from {project_json_url}: {e}")
    else:
        logging.warning(f"Failed to download project.json from {project_json_url}")

    logging.debug(f"Starting download of {len(resources)} resources")
    completed = 1 if index_success else 0
    downloaded = 1 if index_downloaded else 0
    failed = 0 if index_success else 1
    if project_json_success and project_json_downloaded:
        downloaded += 1
    elif not project_json_success:
        failed += 1
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_resource = {
            executor.submit(
                handle_resource, 
                res, 
                session, 
                base_path, 
                base_url_path, 
                base_domain,
                metadata_path
            ): res for res in resources
        }

        for future in as_completed(future_to_resource):
            res = future_to_resource[future]
            try:
                success = future.result()
                if success:
                    completed += 1
                    # Проверяем, был ли файл загружен (временное решение через размер)
                    file_path = os.path.join(base_path, urlparse(res).path.lstrip('/'))
                    if os.path.exists(file_path) and os.path.getsize(file_path) > 0:
                        downloaded += 1
                else:
                    failed += 1
                logging.debug(f"Resource {res} processing completed. Success: {success}")
            except Exception as e:
                failed += 1
                logging.error(f"Error processing resource {res}: {e}")

    logging.info(f"Download completed. Successfully processed: {completed}, Actually downloaded: {downloaded}, Failed: {failed}")
    return completed, downloaded, failed

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: python project_downloader.py <url>")
        sys.exit(1)
    url = sys.argv[1]
    base_path = f"downloaded_games/{url.split('/')[-2]}"
    completed, downloaded, failed = crawl_and_download(url, base_path)
    print(f"Completed: {completed}, Downloaded: {downloaded}, Failed: {failed}")

==================================================
File: vision_query.py
==================================================

#vision_query.py

import google.generativeai as genai
import PIL.Image
import sys
import os
import datetime
import logging
from dotenv import load_dotenv

# Create a directory for the logs if there isn't one
log_dir = "logs"
if not os.path.exists(log_dir):
    os.makedirs(log_dir)

# Setup logging with absolute path and flush
log_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), log_dir, 'vision_query.log')
logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, mode='a', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

# Forced flush after each message
logger = logging.getLogger()
for handler in logger.handlers:
    handler.flush()

def log_with_flush(level, message):
    logger.log(level, message)
    for handler in logger.handlers:
        handler.flush()

# Load environment variables
load_dotenv()
log_with_flush(logging.DEBUG, "Environment variables loaded")

# Configure API key
gemini_api_key = os.getenv("GEMINI_API_KEY")
if not gemini_api_key:
    log_with_flush(logging.ERROR, "GEMINI_API_KEY not found in .env file")
    sys.exit(1)
    
log_with_flush(logging.DEBUG, "API key found, configuring Gemini")
genai.configure(api_key=gemini_api_key)

def analyze_visual_style(image_path):
    log_with_flush(logging.INFO, f"Starting analysis of image: {image_path}")
    
    # Check if file exists
    if not os.path.exists(image_path):
        log_with_flush(logging.ERROR, f"File not found: {image_path}")
        return ""
    
    # Check file size
    file_size = os.path.getsize(image_path)
    log_with_flush(logging.DEBUG, f"File size: {file_size} bytes")
    
    if file_size < 5120:  # 5KB
        error_msg = f"Error: Blank screenshot detected - {image_path}"
        log_with_flush(logging.WARNING, error_msg)
        with open("log.txt", "a") as log_file:
            log_file.write(f"[{datetime.datetime.now()}] {error_msg}\n")
        return ""

    # Load and validate image
    try:
        image = PIL.Image.open(image_path)
        log_with_flush(logging.DEBUG, 
            f"Image loaded successfully: format={image.format}, size={image.size}, mode={image.mode}")
    except Exception as e:
        log_with_flush(logging.ERROR, f"Error loading image: {str(e)}")
        return ""

    # Initialize model
    try:
        log_with_flush(logging.DEBUG, "Initializing Gemini model")
        model = genai.GenerativeModel("gemini-1.5-flash-8b")
        log_with_flush(logging.DEBUG, "Model initialized successfully")
    except Exception as e:
        log_with_flush(logging.ERROR, f"Error initializing model: {str(e)}")
        return f"Model initialization error: {str(e)}"

    # Define prompt
    prompt = """
    You are an expert in visual analysis. Analyze the provided screenshot of a CYOA game and describe it in detail. Focus on the following:
    - Visual style (e.g., cartoonish, realistic, pixel art, etc.)
    - Color palette (dominant colors, background colors, text colors)
    - Objects, characters, or symbols present (describe their appearance, clothing, poses, etc.)
    - Layout and composition (e.g., text placement, image positioning)
    - Any notable details (e.g., specific themes like demons, fantasy, sci-fi, etc.)
    Provide a comprehensive description as if you’re explaining it to someone who cannot see the image. Avoid summarizing; include all relevant visual elements.
    """
    log_with_flush(logging.DEBUG, "Prompt prepared")

    # Generate response with error handling
    try:
        log_with_flush(logging.DEBUG, "Attempting to generate content with model")
        response = model.generate_content([prompt, image])
        log_with_flush(logging.DEBUG, "Response received from model")
        
        if not response.text:
            log_with_flush(logging.WARNING, "Empty response received from model")
            return "Empty response from model"
            
        log_with_flush(logging.INFO, f"Successfully generated description: {response.text[:100]}...")
        return response.text
        
    except Exception as e:
        error_msg = str(e)
        log_with_flush(logging.ERROR, f"Visual analysis error: {error_msg}")
        return error_msg

if __name__ == "__main__":
    log_with_flush(logging.INFO, "Script started")
    
    # Validate command line arguments
    if len(sys.argv) != 2:
        log_with_flush(logging.ERROR, "Invalid number of arguments")
        print("Usage: python vision_query.py <image_path>")
        sys.exit(1)

    image_path = sys.argv[1]
    log_with_flush(logging.INFO, f"Processing image path: {image_path}")
    
    description = analyze_visual_style(image_path)
    
    if not description:
        log_with_flush(logging.WARNING, "No description generated")
    else:
        log_with_flush(logging.INFO, "Description generated successfully")
        
    print(description)
    log_with_flush(logging.INFO, "Script completed")
